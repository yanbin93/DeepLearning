{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please choose recommendation engineer:\n",
      "0->randomRM, 1->topnRM, 2->userBased, 3->itemBased, 4->lfm\n",
      "4\n",
      "The recommendator is lfm\n",
      "F: 10, lmbd:0.0010,alpha:0.001000, precise:7.85%,  recall:4.42%,  coverage:7.74%,  popularity:4.69\n",
      "F: 10, lmbd:0.0010,alpha:0.003000, precise:1.69%,  recall:0.95%,  coverage:10.70%,  popularity:3.63\n",
      "F: 10, lmbd:0.0010,alpha:0.009000, precise:1.36%,  recall:0.77%,  coverage:14.63%,  popularity:3.43\n",
      "F: 10, lmbd:0.0010,alpha:0.018000, precise:3.13%,  recall:1.76%,  coverage:37.12%,  popularity:3.49\n",
      "F: 10, lmbd:0.0010,alpha:0.054000, precise:4.18%,  recall:2.36%,  coverage:51.09%,  popularity:3.85\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*- coding:utf-8 -*-\n",
    "############################\n",
    "#File Name: recommendator.py\n",
    "#Author: yanbin\n",
    "#Mail: yanbin918@gmail.com\n",
    "#Created Time: 2017-08-17 10:\n",
    "from ml_latest_small.data import *\n",
    "from Collaborative_Filter.itemBased import ItemCFModel,ItemCF\n",
    "from Collaborative_Filter.userBased import UserCFModel,UserCF\n",
    "from Collaborative_Filter.lfmBased import lfmModel,lfm\n",
    "from Comparision.randomBased import RandomModel\n",
    "from Comparision.topnBased import topnModel\n",
    "import pickle\n",
    "import os\n",
    "from Evaluation.Evaluator import Evaluator\n",
    "\n",
    "def load_result(path):\n",
    "    pkl_file = open(path, 'rb')\n",
    "    prediction_dict = pickle.load(pkl_file)\n",
    "    # pprint.pprint(data)\n",
    "    pkl_file.close()\n",
    "    return prediction_dict\n",
    "\n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "\n",
    "def itemCF_predict(train_dataset,test_dataset,K):\n",
    "    train_dict = train_dataset.user_item_dict\n",
    "    test_dict = test_dataset.user_item_dict\n",
    "    itemCF = ItemCF()\n",
    "    #triain ItemCF_model\n",
    "    print \"训练基于物品协同过滤算法模型........\"\n",
    "    ItemCF_model=itemCF.train(train_dict)\n",
    "    #recommendation for every user in train_Dataset\n",
    "    users = train_dict.keys()\n",
    "    user_nums = len(users)\n",
    "    i = 0\n",
    "    prediction_dict ={}\n",
    "    for user in users:\n",
    "        prediction_dict[user]=ItemCF_model.recommendation(user,config={'K':K,'N':10}).keys()\n",
    "    #将每位用户预测结果存入本地\n",
    "    output = open(result_pt+itemCF_prediction_result_pt %K, 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(prediction_dict, output)\n",
    "    print \"物品协同过滤推荐算法预测结果存入本地!\"\n",
    "    # Pickle the list using the highest protocol available.\n",
    "    output.close()\n",
    "    return prediction_dict\n",
    "\n",
    "def userCF_predict(train_dataset,test_dataset,K):\n",
    "    train_dict = train_dataset.user_item_dict\n",
    "    test_dict = test_dataset.user_item_dict    \n",
    "    userCF = UserCF()\n",
    "    #triain ItemCF_model\n",
    "    print \"用户基于物品协同过滤算法模型........\"\n",
    "    UserCF_model=userCF.train(train_dict)\n",
    "    #recommendation for every user in train_Dataset\n",
    "    users = train_dict.keys()\n",
    "    user_nums = len(users)\n",
    "    i = 0\n",
    "    prediction_dict ={}\n",
    "    for user in users:\n",
    "        prediction_dict[user]=UserCF_model.recommendation(user,config={'K':K,'N':10}).keys()\n",
    "    #将每位用户预测结果存入本地\n",
    "    output = open(result_pt+userCF_prediction_result_pt %K, 'wb')\n",
    "    # Pickle dictionary using protocol 0.bb\n",
    "    pickle.dump(prediction_dict, output)\n",
    "    print \"用户协同过滤推荐算法预测结果存入本地!\"\n",
    "    # Pickle the list using the highest protocol available.\n",
    "    output.close()\n",
    "    return prediction_dict\n",
    "\n",
    "def random_predict(train_dataset,test_dataset,K):\n",
    "    train_dict = train_dataset.user_item_dict\n",
    "    test_dict = test_dataset.user_item_dict\n",
    "    randomRM = RandomModel(train_dict)\n",
    "    #triain ItemCF_model\n",
    "    print \"基于随机推荐算法模型........\"\n",
    "    #recommendation for every user in train_Dataset\n",
    "    users = train_dict.keys()\n",
    "    user_nums = len(users)\n",
    "    i = 0\n",
    "    prediction_dict ={}\n",
    "    for user in users:\n",
    "        prediction_dict[user]=randomRM.recommendation(user,config={'K':K,'N':10}).keys()\n",
    "    #将每位用户预测结果存入本地\n",
    "    output = open(result_pt+random_prediction_result_pt %K, 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(prediction_dict, output)\n",
    "    print \"基于随机推荐算法预测结果存入本地!\"\n",
    "    # Pickle the list using the highest protocol available.\n",
    "    output.close()\n",
    "    return prediction_dict\n",
    "    \n",
    "def topn_predict(train_dataset,test_dataset):\n",
    "    train_dict = train_dataset.user_item_dict\n",
    "    test_dict = test_dataset.user_item_dict\n",
    "    topn_model = topnModel()\n",
    "    topn_model.fit(train_dict)\n",
    "    #triain ItemCF_model\n",
    "    print \"热门商品推荐算法模型........\"\n",
    "    #recommendation for every user in train_Dataset\n",
    "    users = train_dict.keys()\n",
    "    user_nums = len(users)\n",
    "    i = 0\n",
    "    prediction_dict ={}\n",
    "    for user in users:\n",
    "        prediction_dict[user]=topn_model.recommendation(user,config={'N':10}).keys()\n",
    "    #将每位用户预测结果存入本地\n",
    "    output = open(result_pt+topn_prediction_result_pt, 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(prediction_dict, output)\n",
    "    print \"热门商品推荐算法预测结果存入本地!\"\n",
    "    # Pickle the list using the highest protocol available.\n",
    "    output.close()\n",
    "    return prediction_dict\n",
    "\n",
    "def lfm_predict(train_dataset,test_dataset,F=10,lmbd=0.01,alpha=0.9):\n",
    "    train_dict = train_dataset.user_item_dict\n",
    "    test_dict = test_dataset.user_item_dict\n",
    "    LFM = lfm(F=F,lmbd=lmbd,alpha=alpha,max_iter = 100)\n",
    "    lfm_model= LFM.fit(train_dict)\n",
    "    #triain lfm_model\n",
    "    print \"LFM推荐算法模型........\"\n",
    "    #recommendation for every user in train_Dataset\n",
    "    users = train_dict.keys()\n",
    "    user_nums = len(users)\n",
    "    i = 0\n",
    "    prediction_dict ={}\n",
    "    for user in users:\n",
    "        prediction_dict[user]=lfm_model.recommendation(user,config={'N':10}).keys()\n",
    "    #将每位用户预测结果存入本地\n",
    "    output = open(result_pt+lfm_prediction_result_pt %(F,lmbd,alpha), 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(prediction_dict, output)\n",
    "    print \"LFM推荐算法预测结果存入本地!\"\n",
    "    # Pickle the list using the highest protocol available.\n",
    "    output.close()\n",
    "    return prediction_dict\n",
    "\n",
    "inputPath = \"./ml_latest_small/ratings.csv\"\n",
    "inputPath = '/home/yanbin/data/ml-100k/u.data'\n",
    "result_pt = \"./Predict_Result/\"\n",
    "userCF_prediction_result_pt ='UserCF_Result/%s_usercf_prediction_dict.pkl'\n",
    "itemCF_prediction_result_pt ='ItemCF_Result/%s_itemcf_prediction_dict.pkl'\n",
    "random_prediction_result_pt = 'Random_Result/%s_random_prediction_dict.pkl'\n",
    "topn_prediction_result_pt = 'Topn_Result/topn_prediction_dict.pkl'\n",
    "lfm_prediction_result_pt = 'LFM_Result/%s_%s_%s_lfm_prediction_dict.pkl'\n",
    "recommendation_enginer = {0:random_prediction_result_pt,\n",
    "                          1:topn_prediction_result_pt,\n",
    "                          2:userCF_prediction_result_pt,\n",
    "                          3:itemCF_prediction_result_pt,\n",
    "                          4:lfm_prediction_result_pt}\n",
    "recommendator = {0:'randomRM',1:'topnRM',2:'userCF',3:'itemCF',4:'lfm'}\n",
    "train_dataset,test_dataset = read_data_sets(inputPath,with_split=True,split_char='\\t')\n",
    "Ks = [5,10,20,40,80,160]\n",
    "i = input(\"please choose recommendation engineer:\\n0->randomRM, 1->topnRM, 2->userBased, 3->itemBased, 4->lfm\\n\")\n",
    "assert i  in [0,1,2,3,4] ,\"recomendation enginer not exists\"\n",
    "print \"The recommendator is \"+recommendator[i]\n",
    "if i in [0,2,3]:\n",
    "    for K in Ks:\n",
    "        path = (result_pt+recommendation_enginer[i] %K)\n",
    "        flag = os.path.exists(path)\n",
    "        if flag:prediction_dict = load_result(path)\n",
    "        elif i is 0:\n",
    "                prediction_dict = random_predict(train_dataset,test_dataset,K)\n",
    "        elif i is 2:\n",
    "            prediction_dict = userCF_predict(train_dataset,test_dataset,K)\n",
    "        elif i is 3:\n",
    "            prediction_dict = itemCF_predict(train_dataset,test_dataset,K)\n",
    "        evaluator = Evaluator(train_dataset.user_item_dict,test_dataset.user_item_dict,prediction_dict,N=10)\n",
    "        (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "        print ('K:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "               %(K,precise*100,recall*100,coverage*100,popularity))\n",
    "\n",
    "elif i is 1:\n",
    "    path = (result_pt+recommendation_enginer[i])\n",
    "    flag = os.path.exists(path)\n",
    "    prediction_dict = load_result(path) if flag else\\\n",
    "    topn_predict(train_dataset,test_dataset)\n",
    "    evaluator = Evaluator(train_dataset.user_item_dict,test_dataset.user_item_dict,prediction_dict,N=10)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    print ('precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "           %(precise*100,recall*100,coverage*100,popularity))\n",
    "elif i is 4:\n",
    "    Fs = [10,20]\n",
    "    lmbds = [0.001,0.003,0.009,0.018,0.054,0.108]\n",
    "    alphas = [0.001,0.003,0.009,0.018,0.054,0.108]\n",
    "    for F in Fs:\n",
    "        for lmbd in lmbds:\n",
    "            for alpha in alphas:\n",
    "                path = (result_pt+recommendation_enginer[i] % (F,lmbd,alpha) )\n",
    "                flag = os.path.exists(path)\n",
    "                prediction_dict = load_result(path) if flag else\\\n",
    "                lfm_predict(train_dataset,test_dataset,F=F,lmbd=lmbd,alpha=alpha)\n",
    "                evaluator = Evaluator(train_dataset.user_item_dict,test_dataset.user_item_dict,prediction_dict,N=10)\n",
    "                (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "                print ('F:%3d, lmbd:%.4f, alpha:%4f, precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "                       %(F,lmbd,alpha,precise*100,recall*100,coverage*100,popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomRM\n",
    "随机推荐商品，模型评价指标情况如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  1,  precise:0.28%,  recall:0.11%,  coverage:0.12%,  popularity:1.43\n",
      "K:  2,  precise:0.22%,  recall:0.09%,  coverage:53.84%,  popularity:1.64\n",
      "K:  4,  precise:0.30%,  recall:0.12%,  coverage:54.78%,  popularity:1.65\n",
      "K:  8,  precise:0.21%,  recall:0.08%,  coverage:54.09%,  popularity:1.64\n",
      "K: 16,  precise:0.12%,  recall:0.05%,  coverage:54.90%,  popularity:1.63\n",
      "K: 32,  precise:0.28%,  recall:0.11%,  coverage:54.60%,  popularity:1.63\n"
     ]
    }
   ],
   "source": [
    "for K in Ks:\n",
    "    path = (result_pt+random_prediction_result_pt %K)\n",
    "    flag = os.path.exists(path)\n",
    "    prediction_dict = load_result(path) if flag else \\\n",
    "    random_predict(train_dataset,test_dataset,K)\n",
    "    evaluator = Evaluator(train_dataset.user_item_dict,\n",
    "                          test_dataset.user_item_dict,\n",
    "                          prediction_dict,N=10)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    print ('K:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "           %(K/5,precise*100,recall*100,coverage*100,popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopnRM\n",
    "推荐每个用户，流行度最高N件商品 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 10,  precise:6.96%,  recall:2.79%,  coverage:0.12%,  popularity:5.46\n"
     ]
    }
   ],
   "source": [
    "path = (result_pt+topn_prediction_result_pt)\n",
    "flag = os.path.exists(path)\n",
    "prediction_dict = load_result(path) if flag else \\\n",
    "topn_predict(train_dataset,test_dataset)\n",
    "evaluator = Evaluator(train_dataset.user_item_dict,\n",
    "                      test_dataset.user_item_dict,\n",
    "                      prediction_dict,N=10)\n",
    "(precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "print ('N:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "       %(10,precise*100,recall*100,coverage*100,popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UserCF\n",
    "### 基于用户的协同过滤算法\n",
    "待补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  5,  precise:17.70%,  recall:7.10%,  coverage:9.90%,  popularity:4.59\n",
      "K: 10,  precise:19.97%,  recall:8.01%,  coverage:6.87%,  popularity:4.77\n",
      "K: 20,  precise:21.88%,  recall:8.78%,  coverage:4.81%,  popularity:4.92\n",
      "K: 40,  precise:22.88%,  recall:9.18%,  coverage:3.18%,  popularity:5.04\n",
      "K: 80,  precise:22.18%,  recall:8.89%,  coverage:2.25%,  popularity:5.14\n",
      "K:160,  precise:21.13%,  recall:8.48%,  coverage:1.64%,  popularity:5.23\n"
     ]
    }
   ],
   "source": [
    "for K in Ks:\n",
    "    path = (result_pt+userCF_prediction_result_pt %K)\n",
    "    flag = os.path.exists(path)\n",
    "    prediction_dict = load_result(path) if flag else \\\n",
    "    userCF_predict(train_dataset,test_dataset,K)\n",
    "    evaluator = Evaluator(train_dataset.user_item_dict,\n",
    "                          test_dataset.user_item_dict,\n",
    "                          prediction_dict,N=10)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    print ('K:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "           %(K,precise*100,recall*100,coverage*100,popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ItemCF\n",
    "### 基于物品的协同过滤算法\n",
    "待补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  5,  precise:20.39%,  recall:8.18%,  coverage:8.79%,  popularity:4.58\n",
      "K: 10,  precise:21.91%,  recall:8.79%,  coverage:6.54%,  popularity:4.78\n",
      "K: 20,  precise:21.83%,  recall:8.76%,  coverage:5.46%,  popularity:4.84\n",
      "K: 40,  precise:22.24%,  recall:8.92%,  coverage:4.92%,  popularity:4.87\n",
      "K: 80,  precise:22.47%,  recall:9.01%,  coverage:4.64%,  popularity:4.88\n",
      "K:160,  precise:22.00%,  recall:8.82%,  coverage:4.81%,  popularity:4.83\n"
     ]
    }
   ],
   "source": [
    "for K in Ks:\n",
    "    path = (result_pt+itemCF_prediction_result_pt %K)\n",
    "    flag = os.path.exists(path)\n",
    "    prediction_dict = load_result(path) if flag else \\\n",
    "    itemCF_predict(train_dataset,test_dataset,K)\n",
    "    evaluator = Evaluator(train_dataset.user_item_dict,\n",
    "                          test_dataset.user_item_dict,\n",
    "                          prediction_dict,N=10)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    print ('K:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "           %(K,precise*100,recall*100,coverage*100,popularity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFM\n",
    "###  基于隐语义模型的推荐\n",
    "待补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: 10,  precise:0.46%,  recall:0.19%,  coverage:2.35%,  popularity:1.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-107843384898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult_pt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlfm_prediction_result_pt\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprediction_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;32melse\u001b[0m    \u001b[0mlfm_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     evaluator = Evaluator(train_dataset.user_item_dict,\n\u001b[1;32m      7\u001b[0m                           \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_item_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-6b868cc94e1a>\u001b[0m in \u001b[0;36mlfm_predict\u001b[0;34m(train_dataset, test_dataset, F)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_item_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mLFM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mlfm_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLFM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;31m#triain lfm_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"LFM推荐算法模型........\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/pythonProject/DeepLearning/recommendation/Collaborative_Filter/lfmBased.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dict)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merr_ui\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmbd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merr_ui\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmbd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.9\u001b[0m  \u001b[0;31m# 每次迭代步长要逐步缩小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlfmModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Fs = [10,100]\n",
    "for F in Fs:\n",
    "    path = (result_pt+lfm_prediction_result_pt %F)\n",
    "    flag = os.path.exists(path)\n",
    "    prediction_dict = load_result(path) if flag else\\\n",
    "    lfm_predict(train_dataset,test_dataset,F=F)\n",
    "    evaluator = Evaluator(train_dataset.user_item_dict,\n",
    "                          test_dataset.user_item_dict,\n",
    "                          prediction_dict,N=10)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    print ('F:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f'\n",
    "           %(F,precise*100,recall*100,coverage*100,popularity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
