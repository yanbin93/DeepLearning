{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- user: integer (nullable = true)\n",
      "\n",
      "+----+------+---------+----+\n",
      "|item|rating|timestamp|user|\n",
      "+----+------+---------+----+\n",
      "|2902|   4.0|971581445|4169|\n",
      "|1480|   3.0|973310869|4169|\n",
      "|2521|   2.0|974956577|1941|\n",
      "|3421|   5.0|970049117|4277|\n",
      "|3763|   4.0|974715698|1680|\n",
      "+----+------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "total lines: 1000209\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "#__author__:yanbin\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from operator import add,itemgetter\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pandas import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import math\n",
    "class ItemCFModel(object):  \n",
    "    def __init__(self,df,item_pair_sim=None,spark=None):\n",
    "        self.df = df\n",
    "        self.item_pair_sim = item_pair_sim\n",
    "\n",
    "    def itemSimilarity(self):\n",
    "        # RDD[(uid,(aid,score))] \n",
    "        user_item_score = self.df.rdd.map(lambda x:(x['user'],[x['item'],x['rating']]))\n",
    "        item_score_pair = user_item_score.join(user_item_score)\\\n",
    "                        .map(lambda x:((x[1][0][0],x[1][1][0]),(x[1][0][1],x[1][1][1])))\n",
    "        item_pair_ALL = item_score_pair.map(lambda f:(f[0], f[1][0] * f[1][1])).reduceByKey(add,300)\n",
    "        item_pair_XX_YY = item_pair_ALL.filter(lambda f:f[0][0] == f[0][1])\n",
    "        item_pair_XY = item_pair_ALL.filter(lambda f:f[0][0] != f[0][1])\n",
    "        #RDD[(aid1,score11 * score11 + score21 * score21)] \n",
    "        item_XX_YY = item_pair_XX_YY.map(lambda f:(f[0][0], f[1]))\n",
    "        #RDD(aid1,((aid1,aid2,XY),XX))\n",
    "        item_XY_XX = item_pair_XY.map(lambda f:(f[0][0], (f[0][0], f[0][1], f[1]))).join(item_XX_YY) \n",
    "        #RDD[(aid2,((aid1,aid2,\n",
    "        #           score11 * score12 + score21 * score22,score11 * score11 + score21 * score21),\n",
    "        #           score12 * score12 + score22 * score22))] \n",
    "        item_XY_XX_YY = item_XY_XX.map(lambda f:(f[1][0][1],(f[1][0][0],f[1][0][1],f[1][0][2],f[1][1]))).join(item_XX_YY)  \n",
    "        # item_XY_XX_YY中的(aid1,aid2,XY,XX,YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22,score11 * score11 + score21 * score21,score12 * score12 + score22 * score22)]       \n",
    "        item_pair_XY_XX_YY = item_XY_XX_YY.map(lambda f:(f[1][0][0], f[1][0][1], f[1][0][2], f[1][0][3], f[1][1]))  \n",
    "        # item_pair_XY_XX_YY为(aid1,aid2,XY / math.sqrt(XX * YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22 / math.sqrt((score11 * score11 + score21 * score21)*(score12 * score12 + score22 * score22))] \n",
    "        item_pair_sim = item_pair_XY_XX_YY.map(lambda f :(f[0], (f[1], f[2] / math.sqrt(f[3] * f[4]))))  \n",
    "        return item_pair_sim\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        item_pair_sim = self.itemSimilarity()\n",
    "        item_pair_sim.cache()  \n",
    "        self.item_pair_sim=item_pair_sim\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def recommend(df,item_pair_sim,NN=100,topN =10,Normalization=False):\n",
    "    def itemNN(item_pair_sim,NN=100,Normalization=False):\n",
    "        item_sim = item_pair_sim.filter(lambda f:f[1][1]>0.05)\\\n",
    "                            .groupByKey()\\\n",
    "                            .mapValues(list)\n",
    "        if Normalization:\n",
    "            def norm(x):\n",
    "                m =  __builtin__.max([i[1] for i in x])\n",
    "                l = []\n",
    "                for i in x:\n",
    "                    l.append((i[0],i[1]/m))\n",
    "                return l\n",
    "            item_sim = item_sim.mapValues(lambda x:norm(x))\n",
    "        item_simNN = item_sim.mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:NN])\\\n",
    "                            .collectAsMap()\n",
    "        return item_simNN\n",
    "    \n",
    "    def getOrElse(f,item_sim_bd):\n",
    "        items_sim = item_sim_bd.value.get(f[0][1]) \n",
    "        if items_sim is None:\n",
    "            items_sim = [(\"0\", 0.0)]\n",
    "        for w in items_sim:\n",
    "            yield ((f[0][0],w[0]),w[1]*f[1])\n",
    "            \n",
    "    user_item_score = df.rdd.map(lambda x:((x['user'],x['item']),x['rating']))\n",
    "    item_sim_bd = sc.broadcast(itemNN(item_pair_sim,NN=NN,Normalization=Normalization))\n",
    "#     /* \n",
    "#      * 提取item_sim_user_score为((user,item2),sim * score) \n",
    "#      * RDD[(user,item2),sim * score] \n",
    "#      */  \n",
    "\n",
    "    user_item_simscore = user_item_score.flatMap(lambda f:getOrElse(f,item_sim_bd))\\\n",
    "                                        .filter(lambda f:f[1]> 0.03)  \n",
    "#       /*\n",
    "#      * 聚合user_item_simscore为 (user,（item2,sim1 * score1 + sim2 * score2）)\n",
    "#      * 假设user观看过两个item,评分分别为score1和score2，item2是与user观看过的两个item相似的item,相似度分别为sim1，sim2 \n",
    "#      * RDD[(user,item2),sim1 * score1 + sim2 * score2）)] \n",
    "#      */  \n",
    "    user_item_rank = user_item_simscore.reduceByKey(add,1000)  \n",
    "\n",
    "#     /* \n",
    "#      * 过滤用户已看过的item,并对user_item_rank基于user聚合 \n",
    "#      * RDD[(user,CompactBuffer((item2,rank2）,(item3,rank3)...))] \n",
    "#      */  \n",
    "    user_items_ranks = user_item_rank.subtractByKey(user_item_score)\\\n",
    "                                     .map(lambda f:(f[0][0], (f[0][1], f[1])))\\\n",
    "                                     .groupByKey()  \n",
    "#     /* \n",
    "#      * 对user_items_ranks基于rank降序排序，并提取topN,其中包括用户已观看过的item \n",
    "#      * RDD[(user,ArrayBuffer((item2,rank2）,...,(itemN,rankN)))] \n",
    "#      */  \n",
    "    user_items_ranks_desc = user_items_ranks.mapValues(list)\\\n",
    "                            .mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:topN])\n",
    "    return user_items_ranks_desc\n",
    "\n",
    "from Evaluator import Evaluator\n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    import os\n",
    "    PYSPARK_PYTHON = \"/usr/bin/python2.7\"\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "    conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "    conf.set('spark.yarn.dist.files',\n",
    "            'file:/root/hadoop-2.6/spark/python/lib/pyspark.zip,file:/root/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip')\n",
    "    conf.set('spark.executor.cores','30')\n",
    "    conf.set('spark.executor.memory','95g')\n",
    "    conf.set('spark.executor.instances','8')\n",
    "    conf.set('spark.sql.shuffle.partitions','400')\n",
    "    conf.set('spark.default.parallelism','200')\n",
    "#     conf.set(\"spark.shuffle.file.buffer\",\"128k\").set(\"spark.reducer.maxSizeInFlight\",\"96M\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    sc=spark.sparkContext \n",
    "    sc.setLogLevel('WARN')\n",
    "    start = time.time()\n",
    "    inputPath = 'data/ml-1m/ratings.dat'\n",
    "    schema = StructType([\n",
    "            StructField(\"user\", StringType(), True),\n",
    "            StructField(\"item\", StringType(), True),\n",
    "            StructField(\"rating\", DoubleType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)])\n",
    "#     ratingRdd = sc.textFile(inputPath).map(lambda line:line.split(\"::\"))\\\n",
    "#                     .map(lambda x:(x[0],x[1],float(x[2]),long(x[3])))\n",
    "#     ratingDf = spark.createDataFrame(data=ratingRdd,schema=schema)\n",
    "    #     ratingDf,_ = ratingDf.randomSplit([1.0,9.0],seed=40)\n",
    "    ratingDf = spark.read.parquet('data/ml-1m/user_parquet')\n",
    "    ratingDf = ratingDf.repartition(300)\n",
    "    ratingDf.printSchema()\n",
    "    ratingDf.show(5)\n",
    "    n = ratingDf.count()\n",
    "#     ratingDf = ratingDf.withColumn('score',col('rating')*0+1).select('user','item','score')    \n",
    "    print 'total lines: %s' %n\n",
    "    train,test = ratingDf.randomSplit([4.0,1.0],seed=40)\n",
    "    train.cache()\n",
    "    test.cache()\n",
    "    itemCF = ItemCFModel(df=train,spark=spark)\n",
    "    itemCF.train()\n",
    "#     for NN in [5,10,20,40,60,80,100,120]:\n",
    "    for NN in [100]:\n",
    "        recTopN = recommend(train,itemCF.item_pair_sim,NN=NN,topN=10,Normalization=False)\n",
    "        pre = spark.createDataFrame(data=recTopN.flatMapValues(lambda x:x).map(lambda x:(x[0],x[1][0],x[1][1])),\n",
    "                                    schema=['user','item','rating'])\n",
    "        evaluator = Evaluator(train,test,pre)\n",
    "        (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "        end = time.time()\n",
    "        print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "                %(NN,precise*100,recall*100,coverage*100,popularity,end-start))\n",
    "        pre.write.parquet(partitionBy='user',path='data/ml-1m/result/itemCF/itcf_'+str(NN),mode='ignore')\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:100,  precise:12.40%,  recall:37.42%,  coverage:37.16%,  popularity:6.97, time: 80.6126019955 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "recTopN = recommend(train,itemCF.item_pair_sim,NN=NN,topN=100,Normalization=False)\n",
    "pre = spark.createDataFrame(data=recTopN.flatMapValues(lambda x:x).map(lambda x:(x[0],x[1][0],x[1][1])),\n",
    "                            schema=['user','item','rating'])\n",
    "evaluator = Evaluator(train,test,pre)\n",
    "(precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "end = time.time()\n",
    "print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "        %(NN,precise*100,recall*100,coverage*100,popularity,end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler,VectorAssembler,MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "normalizer = Normalizer(p=2.0, inputCol=\"vector\", outputCol=\"new\")\n",
    "vecAssembler = VectorAssembler(inputCols=[\"rating\"], outputCol=\"vector\")\n",
    "maScaler = MaxAbsScaler(inputCol=\"vector\", outputCol=\"scaled\")\n",
    "standardScaler = StandardScaler(inputCol=\"vector\", outputCol=\"scaled\",withMean=True,withStd=False)\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "pipe = Pipeline(stages=[vecAssembler,standardScaler])\n",
    "scaledPre = pipe.fit(pre).transform(pre)\n",
    "# pre.filter('user=1').withColumn('new',col('rating')*max(col('rating'))+1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+\n",
      "|userId|itemId|rating|timestamp|\n",
      "+------+------+------+---------+\n",
      "|     1|  1193|   5.0|978300760|\n",
      "|     1|   661|   3.0|978302109|\n",
      "|     1|   914|   3.0|978301968|\n",
      "|     1|  3408|   4.0|978300275|\n",
      "|     1|  2355|   5.0|978824291|\n",
      "+------+------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----+--------------------+\n",
      "|itemId|title|              genres|\n",
      "+------+-----+--------------------+\n",
      "|     1| 1995|[0.0,0.0,1.0,1.0,...|\n",
      "|     2| 1995|[0.0,1.0,0.0,1.0,...|\n",
      "|     3| 1995|[0.0,0.0,0.0,0.0,...|\n",
      "|     4| 1995|[0.0,0.0,0.0,0.0,...|\n",
      "|     5| 1995|[0.0,0.0,0.0,0.0,...|\n",
      "+------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+------+---+----------+--------+\n",
      "|userId|gender|age|occupation|zip-code|\n",
      "+------+------+---+----------+--------+\n",
      "|     1|     F|  1|        10|   48067|\n",
      "|     2|     M| 56|        16|   70072|\n",
      "|     3|     M| 25|        15|   55117|\n",
      "|     4|     M| 45|         7|   02460|\n",
      "|     5|     M| 25|        20|   55455|\n",
      "+------+------+---+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36418.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 8289 tasks (10.0 GB) is bigger than spark.driver.maxResultSize (10.0 GB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9a8ea09929e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mposDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mnegDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mposDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0mnegDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegDf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/hadoop-2.6/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/hadoop-2.6/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o36418.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 8289 tasks (10.0 GB) is bigger than spark.driver.maxResultSize (10.0 GB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "#__author__:yanbin\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from operator import add,itemgetter\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pandas import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import *\n",
    "import time,math,os,re\n",
    "PYSPARK_PYTHON = \"/usr/bin/python2.7\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "conf.set('spark.yarn.dist.files',\n",
    "        'file:/root/hadoop-2.6/spark/python/lib/pyspark.zip,file:/root/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip')\n",
    "conf.set('spark.executor.cores','30')\n",
    "conf.set('spark.executor.memory','95g')\n",
    "conf.set('spark.executor.instances','8')\n",
    "conf.set('spark.sql.shuffle.partitions','400')\n",
    "conf.set('spark.default.parallelism','200')\n",
    "conf.set('spark.driver.maxResultSize', '10g')\n",
    "conf.set('spark.scheduler.mode','fair')\n",
    "\n",
    "#     conf.set(\"spark.shuffle.file.buffer\",\"128k\").set(\"spark.reducer.maxSizeInFlight\",\"96M\")\n",
    "spark = SparkSession.builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "sc=spark.sparkContext \n",
    "sc.setLogLevel('WARN')\n",
    "sc.setLocalProperty(\"spark.scheduler.pool\", \"rank\")\n",
    "start = time.time()\n",
    "ratingsPath = 'data/ml-1m/ratings.dat'\n",
    "moviesPath = 'data/ml-1m/movies.dat'\n",
    "usersPath = 'data/ml-1m/users.dat'\n",
    "\n",
    "rating_schema = StructType([\n",
    "                StructField(\"userId\", StringType(), True),\n",
    "                StructField(\"itemId\", StringType(), True),\n",
    "                StructField(\"rating\", DoubleType(), True),\n",
    "                StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "movie_schema = StructType([\n",
    "                StructField(\"itemId\", StringType(), True),\n",
    "                StructField(\"title\", IntegerType(), True),\n",
    "                StructField(\"genres\", ArrayType(FloatType()), True)])\n",
    "\n",
    "user_schema = StructType([\n",
    "                StructField(\"userId\", StringType(), True),\n",
    "                StructField(\"gender\", StringType(), True),\n",
    "                StructField(\"age\", IntegerType(), True),\n",
    "                StructField(\"occupation\", IntegerType(), True),\n",
    "                StructField(\"zip-code\", StringType(), True)])\n",
    "genresDict ={\n",
    " 'Action': 0,\n",
    " 'Adventure': 1,\n",
    " 'Animation': 2,\n",
    " \"Children's\": 3,\n",
    " 'Comedy': 4,\n",
    " 'Crime': 5,\n",
    " 'Documentary': 6,\n",
    " 'Drama': 7,\n",
    " 'Fantasy': 8,\n",
    " 'Film-Noir': 9,\n",
    " 'Horror': 10,\n",
    " 'Musical': 11,\n",
    " 'Mystery': 12,\n",
    " 'Romance': 13,\n",
    " 'Sci-Fi': 14,\n",
    " 'Thriller': 15,\n",
    " 'War': 16,\n",
    " 'Western': 17}\n",
    "#rating 数据DataFrame获取\n",
    "ratingRdd = sc.textFile(ratingsPath).map(lambda line:line.split(\"::\"))\\\n",
    "                .map(lambda x:(x[0],x[1],float(x[2]),long(x[3])))\n",
    "ratingDf = spark.createDataFrame(data=ratingRdd,schema=rating_schema)\n",
    "#movie 属性DataFrame获取\n",
    "\n",
    "\n",
    "def threeTuple(x,genresDict):\n",
    "    arr = np.zeros((18))\n",
    "    for k in x.split('|'):\n",
    "        i = genresDict.get(k)\n",
    "        if  i is not None:arr[i] = 1\n",
    "    return arr.tolist()\n",
    "movieRdd = sc.textFile(moviesPath).map(lambda line:line.split(\"::\"))\\\n",
    "                .map(lambda x:(x[0],int(re.findall(r'[^()]+', x[1])[-1]),Vectors.dense(threeTuple(x[2],genresDict))))\n",
    "movieDf = spark.createDataFrame(data=movieRdd,schema=['itemId','title','genres'])\n",
    "#user 属性DataFrame获取\n",
    "userRdd = sc.textFile(usersPath).map(lambda line:line.split(\"::\"))\\\n",
    "                .map(lambda x:(x[0],x[1],int(x[2]),int(x[3]),x[4]))\n",
    "userDf = spark.createDataFrame(data=userRdd,schema=user_schema)\n",
    "#     ratingDf,_ = ratingDf.randomSplit([1.0,9.0],seed=40)\n",
    "\n",
    "ratingDf.show(5)\n",
    "movieDf.show(5)\n",
    "userDf.show(5)\n",
    "userIds = ratingDf.select('userId').distinct().rdd.map(lambda x:x['userId']).collect()\n",
    "# userIds = [u'1']\n",
    "posDf = spark.createDataFrame(data=[],schema=rating_schema)\n",
    "negDf = spark.createDataFrame(data=[],schema=rating_schema)\n",
    "\n",
    "for userId in userIds:\n",
    "    pos = ratingDf.filter('userId='+userId)\n",
    "    n = pos.count()\n",
    "    posDf = posDf.unionAll(pos)\n",
    "    negDf = negDf.unionAll(ratingDf.filter('userId!='+userId).sample(False,0.1).limit(n))\n",
    "\n",
    "posDf.cache()\n",
    "negDf.cache()\n",
    "posDf.show(5)\n",
    "negDf.show(5)\n",
    "df = posDf.unionAll(negDf)\n",
    "# posDf = posDf.join(movieDf,on='itemId').join(userDf,on='userId')\n",
    "# posDf = posDf.withColumn('label',col('rating')*0)\n",
    "# negDf = negDf.join(movieDf,on='itemId').join(userDf,on='userId')\n",
    "# negDf = negDf.withColumn('label',col('rating')*0+1.0).withColumn('label',col('rating')*0+1.0)\n",
    "\n",
    "df.show(5)    \n",
    "print '训练数据集：',df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- itemId: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: long (nullable = true)\n",
      " |-- genres: vector (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- occupation: integer (nullable = true)\n",
      " |-- zip-code: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '`userId`' given input columns: [item, rating, timestamp, user]; line 1 pos 0;\\n'Filter ('userId = 1)\\n+- Repartition 300, true\\n   +- Relation[item#40501,rating#40502,timestamp#40503L,user#40504] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-309-e7a887ba7066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muserId\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserIds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mposDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratingDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'userId='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnegDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratingDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'userId!='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/hadoop-2.6/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \"\"\"\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/hadoop-2.6/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`userId`' given input columns: [item, rating, timestamp, user]; line 1 pos 0;\\n'Filter ('userId = 1)\\n+- Repartition 300, true\\n   +- Relation[item#40501,rating#40502,timestamp#40503L,user#40504] parquet\\n\""
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n",
    "from pyspark.ml.classification import BinaryLogisticRegressionSummary\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString,HashingTF,Tokenizer,VectorIndexer,VectorAssembler\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "labelInder = StringIndexer(inputCol='label',outputCol='indexedLabel').fit(df)\n",
    "genderInder = StringIndexer(inputCol='gender',outputCol='indexedGender').fit(df)\n",
    "zipInder = StringIndexer(inputCol='zip-code',outputCol='indexedZip').fit(df)\n",
    "normalizer = Normalizer(p=2.0, inputCol=\"vector\", outputCol=\"new\")\n",
    "vecAssembler1 = VectorAssembler(inputCols=['timestamp','title','age','indexedGender','occupation','indexedZip']\n",
    "                               , outputCol=\"feature1\")\n",
    "vecAssembler2 = VectorAssembler(inputCols=['feature1','genres']\n",
    "                               ,outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='label',\\\n",
    "                        maxIter=10,regParam=2,elasticNetParam=0.8)\n",
    "# featureIndex = VectorIndexer(inputCol='features',outputCol='indexedFeatures').fit(df)\n",
    "(traningDf,testDf) = df.randomSplit(seed=40,weights=[0.7,0.3])\n",
    "# labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel',labels=labelInder.labels)\n",
    "lrPipeline = Pipeline().setStages([labelInder,genderInder,zipInder,vecAssembler1,vecAssembler2,lr])\n",
    "lrModel = lrPipeline.fit(traningDf)\n",
    "lrPredictions = lrModel.transform(testDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  用户候选集选出Top100，经排序选出Top10，效果对比\n",
    "### Top100排评分选出Top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------------+\n",
      "|user|item|            rating|\n",
      "+----+----+------------------+\n",
      "|   1|1196| 48.49930907578779|\n",
      "|   1|1270| 47.61567828646564|\n",
      "|   1|1198| 45.08401892251104|\n",
      "|   1| 919| 44.20005863231689|\n",
      "|   1|1210| 42.93014370986168|\n",
      "|   1|1197|41.348620242209684|\n",
      "|   1|1580| 39.99374911460752|\n",
      "|   1|2716|37.961563721223165|\n",
      "|   1| 593|36.941247196018764|\n",
      "|   1|1265| 36.74054918535605|\n",
      "+----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top10 = pre.filter('user=1').sort(desc('rating')).limit(10)\n",
    "top10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:100,  precise:30.00%,  recall:23.08%,  coverage:0.27%,  popularity:7.58, time: 80.6126019955 s\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(train,test.filter('user=1'),top10)\n",
    "(precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "        %(NN,precise*100,recall*100,coverage*100,popularity,end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------------+------+-----+--------------------+------+------+---+----------+--------+\n",
      "|user|item|            rating|itemId|title|              genres|userId|gender|age|occupation|zip-code|\n",
      "+----+----+------------------+------+-----+--------------------+------+------+---+----------+--------+\n",
      "|   1|2294|17.304174172598724|  2294| 1998|[0.0,0.0,1.0,1.0,...|     1|     F|  1|        10|   48067|\n",
      "|   1| 296| 34.08935371931486|   296| 1994|[0.0,0.0,0.0,0.0,...|     1|     F|  1|        10|   48067|\n",
      "|   1|1394|23.186440705224598|  1394| 1987|[0.0,0.0,0.0,0.0,...|     1|     F|  1|        10|   48067|\n",
      "|   1|1500|14.077631919065606|  1500| 1997|[0.0,0.0,0.0,0.0,...|     1|     F|  1|        10|   48067|\n",
      "|   1|2628| 18.93425303048274|  2628| 1999|[1.0,1.0,0.0,0.0,...|     1|     F|  1|        10|   48067|\n",
      "+----+----+------------------+------+-----+--------------------+------+------+---+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "NN:100,  precise:10.00%,  recall:7.69%,  coverage:0.27%,  popularity:6.97, time: -5167.21432781 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preDf = pre.filter('user=1').join(movieDf,pre.item == movieDf.itemId,'inner')\n",
    "preDf = preDf.join(userDf,userDf.userId == preDf.user,'inner')\n",
    "preDf.show(5)\n",
    "lrPredictions = lrModel.transform(preDf)\n",
    "exVec = udf(lambda x:x.values.tolist()[1],DoubleType())\n",
    "lrPredictions = lrPredictions.select('user','item',exVec('probability').alias('probability'),'prediction')\n",
    "top10_new = lrPredictions.sort(desc('probability')).limit(10)\n",
    "evaluator = Evaluator(train,test.filter('user=1'),top10_new)\n",
    "(precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "        %(NN,precise*100,recall*100,coverage*100,popularity,end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(item=u'1270'), Row(item=u'1545'), Row(item=u'2294'), Row(item=u'1961'), Row(item=u'1029'), Row(item=u'1246'), Row(item=u'919'), Row(item=u'527'), Row(item=u'745'), Row(item=u'783'), Row(item=u'1193'), Row(item=u'1197'), Row(item=u'3105')]\n",
      "+----+----+------------------+----------+\n",
      "|user|item|       probability|prediction|\n",
      "+----+----+------------------+----------+\n",
      "|   1|2294|0.5125000000000001|       1.0|\n",
      "|   1| 296|0.5125000000000001|       1.0|\n",
      "|   1|1394|0.5125000000000001|       1.0|\n",
      "|   1|1500|0.5125000000000001|       1.0|\n",
      "|   1|2628|0.5125000000000001|       1.0|\n",
      "|   1|1259|0.5125000000000001|       1.0|\n",
      "|   1|1584|0.5125000000000001|       1.0|\n",
      "|   1|1032|0.5125000000000001|       1.0|\n",
      "|   1|1136|0.5125000000000001|       1.0|\n",
      "|   1|  34|0.5125000000000001|       1.0|\n",
      "|   1|2000|0.5125000000000001|       1.0|\n",
      "|   1|2396|0.5125000000000001|       1.0|\n",
      "|   1|1278|0.5125000000000001|       1.0|\n",
      "|   1|1387|0.5125000000000001|       1.0|\n",
      "|   1|2174|0.5125000000000001|       1.0|\n",
      "|   1|1079|0.5125000000000001|       1.0|\n",
      "|   1|2268|0.5125000000000001|       1.0|\n",
      "|   1| 596|0.5125000000000001|       1.0|\n",
      "|   1|1196|0.5125000000000001|       1.0|\n",
      "|   1|1968|0.5125000000000001|       1.0|\n",
      "|   1|2011|0.5125000000000001|       1.0|\n",
      "|   1|2080|0.5125000000000001|       1.0|\n",
      "|   1|1961|0.5125000000000001|       1.0|\n",
      "|   1| 858|0.5125000000000001|       1.0|\n",
      "|   1|3471|0.5125000000000001|       1.0|\n",
      "|   1|1393|0.5125000000000001|       1.0|\n",
      "|   1|1617|0.5125000000000001|       1.0|\n",
      "|   1|3034|0.5125000000000001|       1.0|\n",
      "|   1|2987|0.5125000000000001|       1.0|\n",
      "|   1|1225|0.5125000000000001|       1.0|\n",
      "|   1| 783|0.5125000000000001|       1.0|\n",
      "|   1|2858|0.5125000000000001|       1.0|\n",
      "|   1| 480|0.5125000000000001|       1.0|\n",
      "|   1| 780|0.5125000000000001|       1.0|\n",
      "|   1| 318|0.5125000000000001|       1.0|\n",
      "|   1|1220|0.5125000000000001|       1.0|\n",
      "|   1| 551|0.5125000000000001|       1.0|\n",
      "|   1|1214|0.5125000000000001|       1.0|\n",
      "|   1|1240|0.5125000000000001|       1.0|\n",
      "|   1|1270|0.5125000000000001|       1.0|\n",
      "|   1|1198|0.5125000000000001|       1.0|\n",
      "|   1|2087|0.5125000000000001|       1.0|\n",
      "|   1|1784|0.5125000000000001|       1.0|\n",
      "|   1|1025|0.5125000000000001|       1.0|\n",
      "|   1|2137|0.5125000000000001|       1.0|\n",
      "|   1|1073|0.5125000000000001|       1.0|\n",
      "|   1|2916|0.5125000000000001|       1.0|\n",
      "|   1|2406|0.5125000000000001|       1.0|\n",
      "|   1|3396|0.5125000000000001|       1.0|\n",
      "|   1|1380|0.5125000000000001|       1.0|\n",
      "|   1|  50|0.5125000000000001|       1.0|\n",
      "|   1|2054|0.5125000000000001|       1.0|\n",
      "|   1|1265|0.5125000000000001|       1.0|\n",
      "|   1| 919|0.5125000000000001|       1.0|\n",
      "|   1|2012|0.5125000000000001|       1.0|\n",
      "|   1| 356|0.5125000000000001|       1.0|\n",
      "|   1| 541|0.5125000000000001|       1.0|\n",
      "|   1|2085|0.5125000000000001|       1.0|\n",
      "|   1|1282|0.5125000000000001|       1.0|\n",
      "|   1|2100|0.5125000000000001|       1.0|\n",
      "|   1| 110|0.5125000000000001|       1.0|\n",
      "|   1|2640|0.5125000000000001|       1.0|\n",
      "|   1| 527|0.5125000000000001|       1.0|\n",
      "|   1| 953|0.5125000000000001|       1.0|\n",
      "|   1|3255|0.5125000000000001|       1.0|\n",
      "|   1|1610|0.5125000000000001|       1.0|\n",
      "|   1|1307|0.5125000000000001|       1.0|\n",
      "|   1|2571|0.5125000000000001|       1.0|\n",
      "|   1| 912|0.5125000000000001|       1.0|\n",
      "|   1| 364|0.5125000000000001|       1.0|\n",
      "|   1|1580|0.5125000000000001|       1.0|\n",
      "|   1|2997|0.5125000000000001|       1.0|\n",
      "|   1|1200|0.5125000000000001|       1.0|\n",
      "|   1|1947|0.5125000000000001|       1.0|\n",
      "|   1|2470|0.5125000000000001|       1.0|\n",
      "|   1|1210|0.5125000000000001|       1.0|\n",
      "|   1|2716|0.5125000000000001|       1.0|\n",
      "|   1| 380|0.5125000000000001|       1.0|\n",
      "|   1|1291|0.5125000000000001|       1.0|\n",
      "|   1|3253|0.5125000000000001|       1.0|\n",
      "|   1| 592|0.5125000000000001|       1.0|\n",
      "|   1|1923|0.5125000000000001|       1.0|\n",
      "|   1| 593|0.5125000000000001|       1.0|\n",
      "|   1| 377|0.5125000000000001|       1.0|\n",
      "|   1| 589|0.5125000000000001|       1.0|\n",
      "|   1| 457|0.5125000000000001|       1.0|\n",
      "|   1|1036|0.5125000000000001|       1.0|\n",
      "|   1|2291|0.5125000000000001|       1.0|\n",
      "|   1|  39|0.5125000000000001|       1.0|\n",
      "|   1|1029|0.5125000000000001|       1.0|\n",
      "|   1|3418|0.5125000000000001|       1.0|\n",
      "|   1|1197|0.5125000000000001|       1.0|\n",
      "|   1|1246|0.5125000000000001|       1.0|\n",
      "|   1|1704|0.5125000000000001|       1.0|\n",
      "|   1|1517|0.5125000000000001|       1.0|\n",
      "|   1|3448|0.5125000000000001|       1.0|\n",
      "|   1|2081|0.5125000000000001|       1.0|\n",
      "|   1| 590|0.5125000000000001|       1.0|\n",
      "|   1|2096|0.5125000000000001|       1.0|\n",
      "|   1|2078|0.5125000000000001|       1.0|\n",
      "+----+----+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print test.filter('user=1').select('item').collect()\n",
    "lrPredictions.sort(desc('probability')).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [[ 0.          0.00089466 -0.04180277  0.1691317  -0.00161059  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]] \n",
      "Intercept:  [0.000152405445756] \n",
      "numClasses:  2 \n",
      "numFeatures:  24\n",
      "('Test Error = ', 0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n",
    "from pyspark.ml.classification import BinaryLogisticRegressionSummary\n",
    "mEvaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction')\n",
    "mlrAccuracy = mEvaluator.evaluate(lrPredictions)\n",
    "mlrModel = lrModel\n",
    "print \"Coefficients: \" , mlrModel.coefficientMatrix.toArray(),\"\\n\"\\\n",
    "        \"Intercept: \",mlrModel.interceptVector,\"\\n\"\\\n",
    "        \"numClasses: \",mlrModel.numClasses,\"\\n\"\\\n",
    "        \"numFeatures: \",mlrModel.numFeatures\n",
    "mlrAccuracy = mEvaluator.evaluate(lrPredictions)\n",
    "print ('Test Error = ',1-mlrAccuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
