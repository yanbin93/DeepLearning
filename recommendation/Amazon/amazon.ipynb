{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Found 11 items',\n",
       " '-rw-r--r--   2 root supergroup      87.8 G 2017-10-28 00:54 /user/root/data/amazon/complete.json',\n",
       " 'drwxr-xr-x   - root supergroup           0 2017-10-31 04:33 /user/root/data/amazon/complete_csv',\n",
       " 'drwxr-xr-x   - root supergroup           0 2017-10-31 23:42 /user/root/data/amazon/complete_sample',\n",
       " '-rw-r--r--   1 root supergroup       3.1 G 2017-10-27 07:29 /user/root/data/amazon/item_dedup.csv',\n",
       " '-rw-r--r--   1 root supergroup      54.3 G 2017-10-27 07:34 /user/root/data/amazon/item_dedup.json',\n",
       " 'drwxr-xr-x   - root supergroup           0 2017-10-31 04:16 /user/root/data/amazon/review_Books',\n",
       " 'drwxr-xr-x   - root supergroup           0 2017-11-02 09:16 /user/root/data/amazon/review_csv',\n",
       " '-rw-r--r--   2 root supergroup      18.4 G 2017-10-30 23:50 /user/root/data/amazon/reviews_Books.json',\n",
       " '-rw-r--r--   1 root supergroup      82.9 K 2017-10-27 07:08 /user/root/data/amazon/sample.json',\n",
       " '-rw-r--r--   2 root supergroup      54.3 G 2017-10-31 02:00 /user/root/data/amazon/user_dedup.json',\n",
       " '-rw-r--r--   2 root supergroup       4.7 M 2017-10-30 23:50 /user/root/data/amazon/user_sample.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !! hadoop fs -rm -r /user/root/data/amazon/review_csv\n",
    "!! hadoop fs -ls -h /user/root/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|         user|      item|rating|\n",
      "+-------------+----------+------+\n",
      "|AFS81Q840AOHC|B00I3MQNWG|   5.0|\n",
      "|AFT8FTOITPZG5|044021002X|   4.0|\n",
      "|AFU4E0FKGUQ7A|B00005JLG2|   3.0|\n",
      "|AFV2584U13XP3|B005NQ5LPK|   5.0|\n",
      "|  AFVQZQ8PW0L|0345495152|   5.0|\n",
      "+-------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "total lines: 142831980\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 8 (reduceByKey at <ipython-input-1-ab6d2672c546>:29) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to node3/172.20.128.162:38375\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:361)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:336)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\nCaused by: java.io.IOException: Failed to connect to node3/172.20.128.162:38375\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)\n\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: 拒绝连接: node3/172.20.128.162:38375\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)\n\t... 2 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1262)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1647)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab6d2672c546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mitemCF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mrecTopN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitemCF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_pair_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         pre = spark.createDataFrame(data=recTopN.flatMapValues(lambda x:x).map(lambda x:(x[0],x[1][0],x[1][1])),\n\u001b[1;32m    160\u001b[0m                                     schema=['user','item','rating'])\n",
      "\u001b[0;32m<ipython-input-1-ab6d2672c546>\u001b[0m in \u001b[0;36mrecommend\u001b[0;34m(df, item_pair_sim, NN, topN, Normalization)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0muser_item_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mitem_sim_bd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_pair_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;31m#     /*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#      * 提取item_sim_user_score为((user,item2),sim * score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ab6d2672c546>\u001b[0m in \u001b[0;36mitemNN\u001b[0;34m(item_pair_sim, NN, Normalization)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mitem_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mitem_simNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem_simNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/hadoop-2.6/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m         \"\"\"\n\u001b[0;32m-> 1569\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/hadoop-2.6/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \"\"\"\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/hadoop-2.6/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31m<type 'str'>\u001b[0m: (<type 'exceptions.UnicodeEncodeError'>, UnicodeEncodeError('ascii', u'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 8 (reduceByKey at <ipython-input-1-ab6d2672c546>:29) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to node3/172.20.128.162:38375\\n\\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:361)\\n\\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:336)\\n\\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)\\n\\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\\n\\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\\n\\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\\n\\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\\n\\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\\n\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\\n\\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\\n\\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\\n\\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)\\n\\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)\\n\\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)\\n\\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\\nCaused by: java.io.IOException: Failed to connect to node3/172.20.128.162:38375\\n\\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)\\n\\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)\\n\\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)\\n\\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\\n\\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\\n\\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)\\n\\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\\n\\tat java.lang.Thread.run(Thread.java:745)\\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: \\u62d2\\u7edd\\u8fde\\u63a5: node3/172.20.128.162:38375\\n\\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\\n\\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\\n\\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)\\n\\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)\\n\\t... 2 more\\n\\n\\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\\n\\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\\n\\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1262)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1647)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\\n\\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\\n\\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\\n\\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:280)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n', 2937, 2941, 'ordinal not in range(128)'))"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "#__author__:yanbin\n",
    "from pyspark import SparkContext,SparkConf,StorageLevel\n",
    "from operator import add,itemgetter\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pandas import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import math\n",
    "class ItemCF_spark(object):  \n",
    "    def __init__(self,df,item_pair_sim=None,spark=None,topN=10,NN=100):\n",
    "        self.spark = spark\n",
    "        if (self.spark is None):\n",
    "            conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "            self.spark = SparkSession.builder\\\n",
    "                .config(conf=conf) \\\n",
    "                .enableHiveSupport()\\\n",
    "                .getOrCreate()\n",
    "        sc = spark.sparkContext \n",
    "        self.df = df\n",
    "        self.item_pair_sim = item_pair_sim\n",
    "        self.topN = topN\n",
    "        self.NN = NN\n",
    "\n",
    "    def itemSimilarity(self):\n",
    "        # RDD[(uid,(aid,score))] \n",
    "        user_item_score = self.df.rdd.map(lambda x:(x[0],[x[1],x[2]]))\n",
    "        item_score_pair = user_item_score.join(user_item_score)\\\n",
    "                        .map(lambda x:((x[1][0][0],x[1][1][0]),(x[1][0][1],x[1][1][1])))\n",
    "        item_pair_ALL = item_score_pair.map(lambda f:(f[0], f[1][0] * f[1][1])).reduceByKey(add,300)\n",
    "        item_pair_XX_YY = item_pair_ALL.filter(lambda f:f[0][0] == f[0][1])\n",
    "        item_pair_XY = item_pair_ALL.filter(lambda f:f[0][0] != f[0][1])\n",
    "        #RDD[(aid1,score11 * score11 + score21 * score21)] \n",
    "        item_XX_YY = item_pair_XX_YY.map(lambda f:(f[0][0], f[1]))\n",
    "        #RDD(aid1,((aid1,aid2,XY),XX))\n",
    "        item_XY_XX = item_pair_XY.map(lambda f:(f[0][0], (f[0][0], f[0][1], f[1]))).join(item_XX_YY) \n",
    "        #RDD[(aid2,((aid1,aid2,\n",
    "        #           score11 * score12 + score21 * score22,score11 * score11 + score21 * score21),\n",
    "        #           score12 * score12 + score22 * score22))] \n",
    "        item_XY_XX_YY = item_XY_XX.map(lambda f:(f[1][0][1],(f[1][0][0],f[1][0][1],f[1][0][2],f[1][1]))).join(item_XX_YY)  \n",
    "        # item_XY_XX_YY中的(aid1,aid2,XY,XX,YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22,score11 * score11 + score21 * score21,score12 * score12 + score22 * score22)]       \n",
    "        item_pair_XY_XX_YY = item_XY_XX_YY.map(lambda f:(f[1][0][0], f[1][0][1], f[1][0][2], f[1][0][3], f[1][1]))  \n",
    "        # item_pair_XY_XX_YY为(aid1,aid2,XY / math.sqrt(XX * YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22 / math.sqrt((score11 * score11 + score21 * score21)*(score12 * score12 + score22 * score22))] \n",
    "        item_pair_sim = item_pair_XY_XX_YY.map(lambda f :(f[0], (f[1], f[2] / math.sqrt(f[3] * f[4]))))  \n",
    "        return item_pair_sim\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        item_pair_sim = self.itemSimilarity()\n",
    "        item_pair_sim.persist(storageLevel=StorageLevel(True, True, False, True, 1))\n",
    "        self.item_pair_sim=item_pair_sim\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def recommend(df,item_pair_sim,NN=100,topN =10,Normalization=False):\n",
    "    def itemNN(item_pair_sim,NN=100,Normalization=False):\n",
    "        item_sim = item_pair_sim.filter(lambda f:f[1][1]>0.05)\\\n",
    "                            .groupByKey()\\\n",
    "                            .mapValues(list)\n",
    "        if Normalization:\n",
    "            def norm(x):\n",
    "                m =  __builtin__.max([i[1] for i in x])\n",
    "                l = []\n",
    "                for i in x:\n",
    "                    l.append((i[0],i[1]/m))\n",
    "                return l\n",
    "            item_sim = item_sim.mapValues(lambda x:norm(x))\n",
    "        item_simNN = item_sim.mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:NN])\\\n",
    "                            .collectAsMap()\n",
    "        return item_simNN\n",
    "    \n",
    "    def getOrElse(f,item_sim_bd):\n",
    "        items_sim = item_sim_bd.value.get(f[0][1]) \n",
    "        if items_sim is None:\n",
    "            items_sim = [(\"0\", 0.0)]\n",
    "        for w in items_sim:\n",
    "            yield ((f[0][0],w[0]),w[1]*f[1])\n",
    "            \n",
    "    user_item_score = df.rdd.map(lambda x:((x[0],x[1]),x[2]))\n",
    "    item_sim_bd = sc.broadcast(itemNN(item_pair_sim,NN=NN,Normalization=Normalization))\n",
    "#     /* \n",
    "#      * 提取item_sim_user_score为((user,item2),sim * score) \n",
    "#      * RDD[(user,item2),sim * score] \n",
    "#      */  \n",
    "\n",
    "    user_item_simscore = user_item_score.flatMap(lambda f:getOrElse(f,item_sim_bd))\\\n",
    "                                        .filter(lambda f:f[1]> 0.03)  \n",
    "#       /*\n",
    "#      * 聚合user_item_simscore为 (user,（item2,sim1 * score1 + sim2 * score2）)\n",
    "#      * 假设user观看过两个item,评分分别为score1和score2，item2是与user观看过的两个item相似的item,相似度分别为sim1，sim2 \n",
    "#      * RDD[(user,item2),sim1 * score1 + sim2 * score2）)] \n",
    "#      */  \n",
    "    user_item_rank = user_item_simscore.reduceByKey(add,1000)  \n",
    "\n",
    "#     /* \n",
    "#      * 过滤用户已看过的item,并对user_item_rank基于user聚合 \n",
    "#      * RDD[(user,CompactBuffer((item2,rank2）,(item3,rank3)...))] \n",
    "#      */  \n",
    "    user_items_ranks = user_item_rank.subtractByKey(user_item_score)\\\n",
    "                                     .map(lambda f:(f[0][0], (f[0][1], f[1])))\\\n",
    "                                     .groupByKey()  \n",
    "#     /* \n",
    "#      * 对user_items_ranks基于rank降序排序，并提取topN,其中包括用户已观看过的item \n",
    "#      * RDD[(user,ArrayBuffer((item2,rank2）,...,(itemN,rankN)))] \n",
    "#      */  \n",
    "    user_items_ranks_desc = user_items_ranks.mapValues(list)\\\n",
    "                            .mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:topN])\n",
    "    return user_items_ranks_desc\n",
    "\n",
    "from Evaluator import Evaluator\n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    import os\n",
    "    PYSPARK_PYTHON = \"/usr/bin/python2.7\"\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "    conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "    conf.set('spark.executor.cores','30')\n",
    "    conf.set('spark.executor.memory','90g')\n",
    "    conf.set('spark.executor.instances','4')\n",
    "    conf.set('spark.sql.shuffle.partitions','400')\n",
    "    conf.set('spark.default.parallelism','200')\n",
    "    conf.set('spark.scheduler.mode','fair')\n",
    "    conf.set(\"spark.shuffle.file.buffer\",\"128k\")\n",
    "    conf.set(\"spark.reducer.maxSizeInFlight\",\"96M\")\n",
    "    conf.set(\"spark.yarn.executor.memoryOverhead\",\"2g\")\n",
    "    conf.set(\"spark.shuffle.memoryFraction\" ,\"0.1\")\n",
    "    conf.set(\"spark.yarn.scheduler.heartbeat.interval\",\"7200000ms\")\n",
    "    conf.set(\"spark.executor.heartbeatInterval\",\"7200000ms\")\n",
    "    conf.set(\"spark.network.timeout\", \"7200000ms\")\n",
    "    conf.set(\"spark.memory.storageFraction\",\"0.25\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    sc=spark.sparkContext \n",
    "    start = time.time()\n",
    "    inputPath = \"data/amazon/rating_parquet\"\n",
    "#     inputPath = \"data/amazon/choosed_parquet\"\n",
    "    ratingDf = spark.read.parquet(inputPath)\n",
    "#     ratingDf,_ = ratingDf.randomSplit([1.0,9.0],seed=40)\n",
    "    ratingDf = ratingDf.repartition(300)\n",
    "#     ratingDf = ratingDf.select('user','item','rating')\n",
    "#     user_count = ratingDf.groupBy('user').agg(count('*').alias('user_count')).filter('user_count>100')\n",
    "#     item_count = ratingDf.groupBy('item').agg(count('*').alias('item_count')).filter('item_count>100')\n",
    "#     ratingDf = ratingDf.join(user_count,on='user',how='inner')\n",
    "#     ratingDf = ratingDf.join(item_count,on='item',how='inner')\n",
    "    ratingDf.printSchema()\n",
    "    ratingDf.show(5)\n",
    "    n = ratingDf.count()\n",
    "#     ratingDf = ratingDf.withColumn('score',col('rating')*0+1).select('user','item','score')    \n",
    "    print 'total lines: %s' %n\n",
    "    train,test = ratingDf.randomSplit([4.0,1.0],seed=40)\n",
    "    train.persist(storageLevel=StorageLevel(True, True, False, True, 1))\n",
    "    test.persist(storageLevel=StorageLevel(True, True, False, True, 1))\n",
    "    itemCF = ItemCF_spark(df=train,spark=spark)\n",
    "    itemCF.train()\n",
    "    for NN in [5,10,20,40,60,80,100,120]:\n",
    "        recTopN = recommend(train,itemCF.item_pair_sim,NN=NN,topN=10,Normalization=True)\n",
    "        pre = spark.createDataFrame(data=recTopN.flatMapValues(lambda x:x).map(lambda x:(x[0],x[1][0],x[1][1])),\n",
    "                                    schema=['user','item','rating'])\n",
    "        evaluator = Evaluator(train,test,pre)\n",
    "        (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "        end = time.time()\n",
    "        print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "                %(NN,precise*100,recall*100,coverage*100,popularity,end-start))\n",
    "        pre.write.parquet(partitionBy='user',path='data/amazon/result/itemCF/itcf_'+str(NN),mode='ignore')\n",
    "        start = time.time()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()\n",
    "# MEMORY_AND_DISK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inputPath = \"data/amazon/complete_csv\"\n",
    "schema = StructType([\n",
    "        StructField(\"user\", StringType(), True),\n",
    "        StructField(\"item\", StringType(), True),\n",
    "        StructField(\"rating\", DoubleType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"summary\", StringType(), True)])\n",
    "ratingDf = spark.read.csv(inputPath,header=True,schema=schema)\n",
    "# ratingDf = ratingDf.select('user','item','rating')\n",
    "# ratingDf.printSchema()\n",
    "# ratingDf.show(5)\n",
    "# n = ratingDf.count()\n",
    "# print 'total lines: %s' %n\n",
    "# ratingDf,_ = ratingDf.randomSplit([1.0,999.0])\n",
    "# n = ratingDf.count()\n",
    "# print 'total lines after reduce: %s' %n\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.ml.pipeline import Pipeline,PipelineModel\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# from pyspark.ml.recommendation import ALS\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import *  \n",
    "# trainDf,testDf = ratingDf.randomSplit([9.0,1.0])\n",
    "# userIndexer = StringIndexer(inputCol=\"user\",outputCol=\"userid\",handleInvalid=\"error\").fit(ratingDf)\n",
    "# itemIndexer = StringIndexer(inputCol=\"item\",outputCol=\"itemid\").fit(ratingDf)\n",
    "# alsModel = ALS(rank=10,userCol=\"userid\",itemCol=\"itemid\",ratingCol=\"rating\",maxIter=5,regParam=0.1)\n",
    "# pipeAls = Pipeline(stages=[userIndexer,itemIndexer,alsModel])\n",
    "# pipeAlsModel = pipeAls.fit(trainDf)\n",
    "# preTrain = pipeAlsModel.transform(trainDf)\n",
    "# preTest= pipeAlsModel.transform(testDf)\n",
    "# preTrain.show(10)\n",
    "# preTest.show(10)\n",
    "# end = time.time()\n",
    "# print 'spend %s s' %(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "n = ratingDf.count()\n",
    "ui_count  = ratingDf.agg(count('*').alias('ui'))\n",
    "user_count = ratingDf.groupBy('user').agg(count('*').alias('user_count'))\n",
    "item_count = ratingDf.groupBy('item').agg(count('*').alias('item_count'))\n",
    "count_union = user_count.describe('user_count')\\\n",
    "          .join(item_count.describe('item_count'),on='summary',how='inner')\n",
    "count_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse is 4.78507940484\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator()\\\n",
    "      .setMetricName(\"rmse\")\\\n",
    "      .setLabelCol(\"rating\")\\\n",
    "      .setPredictionCol(\"prediction\")\n",
    "rmse = evaluator.evaluate(preTest.select('userid','itemid','rating','prediction').na.drop())\n",
    "print 'rmse is %s' %rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ratingDf,_ = ratingDf.randomSplit([1.0,99999.0])\n",
    "# ratingDf.count()\n",
    "ratingDf.write.csv(path='data/amazon/complete_sample',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_csv=ratingDf.select(concat_ws('-',ratingDf.user,ratingDf.item).alias('ui'),\n",
    "                ratingDf.rating,\n",
    "                ratingDf.summary,\n",
    "                ratingDf.text)\n",
    "review_csv.write.csv(path='data/amazon/review_csv',header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
