{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopN热门商品推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "+----+----+------+---------+\n",
      "|user|item|rating|timestamp|\n",
      "+----+----+------+---------+\n",
      "|   4|2028|   5.0|978294230|\n",
      "|   7|1270|   4.0|978234581|\n",
      "|  10|   2|   5.0|979168267|\n",
      "|  10|1654|   5.0|979168346|\n",
      "|  13|1204|   5.0|978202201|\n",
      "+----+----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "total lines: 1000209\n",
      "precise:9.26%,  recall:2.79%,  coverage:0.27%,  popularity:7.71, time: 101.981282949 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from operator import add,itemgetter\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pandas import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import *\n",
    "import time,math,os\n",
    "from Evaluator import Evaluator\n",
    "class topNModel(object):\n",
    "    def __init__(self,df,topNDf=None):\n",
    "        self.topNDf = topNDf\n",
    "        self.df = df\n",
    "        \n",
    "    def train(self):\n",
    "        item_count = self.df.groupBy('item').agg(count('*').alias('item_count'))\n",
    "        self.topNDf = item_count.sort(desc('item_count')).limit(10).select('item')\n",
    "    \n",
    "    def recommend(self):\n",
    "        userIds = ratingDf.select('user').distinct()\n",
    "        return userIds.crossJoin(self.topNDf)\n",
    "    \n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "#     popularity = 5\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "                                                 \n",
    "if __name__ == \"__main__\": \n",
    "    PYSPARK_PYTHON = \"/usr/bin/python2.7\"\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "    conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "    conf.set('spark.yarn.dist.files',\n",
    "            'file:/root/hadoop-2.6/spark/python/lib/pyspark.zip,file:/root/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip')\n",
    "    conf.set('spark.executor.cores','30')\n",
    "    conf.set('spark.executor.memory','95g')\n",
    "    conf.set('spark.executor.instances','8')\n",
    "    conf.set('spark.sql.shuffle.partitions','400')\n",
    "    conf.set('spark.default.parallelism','200')\n",
    "#     conf.set(\"spark.shuffle.file.buffer\",\"128k\").set(\"spark.reducer.maxSizeInFlight\",\"96M\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    sc=spark.sparkContext \n",
    "    sc.setLogLevel('WARN')\n",
    "    start = time.time()\n",
    "    inputPath = 'data/ml-1m/ratings.dat'\n",
    "    schema = StructType([\n",
    "            StructField(\"user\", StringType(), True),\n",
    "            StructField(\"item\", StringType(), True),\n",
    "            StructField(\"rating\", DoubleType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)])\n",
    "    ratingRdd = sc.textFile(inputPath).map(lambda line:line.split(\"::\"))\\\n",
    "                    .map(lambda x:(x[0],x[1],float(x[2]),long(x[3])))\n",
    "    ratingDf = spark.createDataFrame(data=ratingRdd,schema=schema)\n",
    "    #     ratingDf,_ = ratingDf.randomSplit([1.0,9.0],seed=40)\n",
    "    ratingDf = ratingDf.repartition(300)\n",
    "    ratingDf.printSchema()\n",
    "    ratingDf.show(5)\n",
    "    n = ratingDf.count()\n",
    "#     ratingDf = ratingDf.withColumn('score',col('rating')*0+1).select('user','item','score')    \n",
    "    print 'total lines: %s' %n\n",
    "    train,test = ratingDf.randomSplit([4.0,1.0],seed=40)\n",
    "    train.cache()\n",
    "    test.cache()\n",
    "    topModel = topNModel(train)\n",
    "    topModel.train()\n",
    "    pre = topModel.recommend()\n",
    "    evaluator = Evaluator(train,test,pre)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    end = time.time()\n",
    "    print ('precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "            %(precise*100,recall*100,coverage*100,popularity,end-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于随机方法推荐系统 spark实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from operator import add,itemgetter\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pandas import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import *\n",
    "import time,math,os\n",
    "from Evaluator import Evaluator\n",
    "class randomModel(object):\n",
    "    def __init__(self,df,topNDf=None):\n",
    "        self.topNDf = topNDf\n",
    "        self.df = df\n",
    "        \n",
    "    def train(self):\n",
    "        item_count = self.df.groupBy('item').agg(count('*').alias('item_count'))\n",
    "        self.topNDf = item_count.sort(desc('item_count')).limit(10).select('item')\n",
    "    \n",
    "    def recommend(self):\n",
    "        itemIds = ratingDf.select('item').distinct()\n",
    "        userIds = ratingDf.select('user').distinct()\n",
    "        df = spark.createDataFrame(data=[],schema=itemIds.schema)\n",
    "        n = userIds.count()\n",
    "        for i in range(n):\n",
    "            df = df.union(itemIds.sample(True,0.8,42).limit(10)\n",
    "           \n",
    "        return userIds.crossJoin(self.topNDf)\n",
    "    \n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "#     popularity = 5\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "                                                 \n",
    "if __name__ == \"__main__\": \n",
    "    PYSPARK_PYTHON = \"/usr/bin/python2.7\"\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "    conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "    conf.set('spark.yarn.dist.files',\n",
    "            'file:/root/hadoop-2.6/spark/python/lib/pyspark.zip,file:/root/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip')\n",
    "    conf.set('spark.executor.cores','30')\n",
    "    conf.set('spark.executor.memory','95g')\n",
    "    conf.set('spark.executor.instances','8')\n",
    "    conf.set('spark.sql.shuffle.partitions','400')\n",
    "    conf.set('spark.default.parallelism','200')\n",
    "#     conf.set(\"spark.shuffle.file.buffer\",\"128k\").set(\"spark.reducer.maxSizeInFlight\",\"96M\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    sc=spark.sparkContext \n",
    "    sc.setLogLevel('WARN')\n",
    "    start = time.time()\n",
    "    inputPath = 'data/ml-1m/ratings.dat'\n",
    "    schema = StructType([\n",
    "            StructField(\"user\", StringType(), True),\n",
    "            StructField(\"item\", StringType(), True),\n",
    "            StructField(\"rating\", DoubleType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)])\n",
    "    ratingRdd = sc.textFile(inputPath).map(lambda line:line.split(\"::\"))\\\n",
    "                    .map(lambda x:(x[0],x[1],float(x[2]),long(x[3])))\n",
    "    ratingDf = spark.createDataFrame(data=ratingRdd,schema=schema)\n",
    "    #     ratingDf,_ = ratingDf.randomSplit([1.0,9.0],seed=40)\n",
    "    ratingDf = ratingDf.repartition(300)\n",
    "    ratingDf.printSchema()\n",
    "    ratingDf.show(5)\n",
    "    n = ratingDf.count()\n",
    "#     ratingDf = ratingDf.withColumn('score',col('rating')*0+1).select('user','item','score')    \n",
    "    print 'total lines: %s' %n\n",
    "    train,test = ratingDf.randomSplit([4.0,1.0],seed=40)\n",
    "    train.cache()\n",
    "    test.cache()\n",
    "    topModel = topNModel(train)\n",
    "    topModel.train()\n",
    "    pre = topModel.recommend()\n",
    "    evaluator = Evaluator(train,test,pre)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    end = time.time()\n",
    "    print ('precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "            %(precise*100,recall*100,coverage*100,popularity,end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|count|\n",
      "+---+-----+\n",
      "|  0|    4|\n",
      "|  1|    3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
    "sampled = dataset.sampleBy(\"key\", fractions={0: 0.2, 1: 0.2})\n",
    "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  基于物品的协同过滤算法 本地实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练基于物品协同过滤算法模型........\n",
      "NN:  5,  precise:29.88%,  recall:9.02%,  coverage:20.26%,  popularity:7.23, time: 1608.23068404 s\n",
      "NN: 10,  precise:30.92%,  recall:9.33%,  coverage:17.82%,  popularity:7.31, time: 1410.09897399 s\n",
      "NN: 20,  precise:30.29%,  recall:9.14%,  coverage:15.34%,  popularity:7.37, time: 1415.47815514 s\n",
      "NN: 40,  precise:29.35%,  recall:8.86%,  coverage:13.69%,  popularity:7.42, time: 1427.98589611 s\n",
      "NN: 60,  precise:29.09%,  recall:8.78%,  coverage:12.74%,  popularity:7.43, time: 1418.730021 s\n",
      "NN: 80,  precise:28.83%,  recall:8.70%,  coverage:12.06%,  popularity:7.43, time: 1423.06928992 s\n",
      "NN:100,  precise:28.60%,  recall:8.63%,  coverage:11.57%,  popularity:7.43, time: 1416.06087899 s\n",
      "NN:120,  precise:28.44%,  recall:8.59%,  coverage:11.52%,  popularity:7.42, time: 1424.51447105 s\n"
     ]
    }
   ],
   "source": [
    "##### !/usr/bin/env python\n",
    "#-*- coding:utf-8 -*-\n",
    "############################\n",
    "#File Name: recommendator.py\n",
    "#Author: yanbin\n",
    "#Mail: yanbin918@gmail.com\n",
    "#Created Time: 2017-08-17 10:\n",
    "'''\n",
    "inputPath : 文件数据路径\n",
    "split_char ： 文件数据每行分隔符（分隔后取前两个作为用户和物品）\n",
    "读取文件生成dataset类\n",
    "dataset.user_item_dict：{user1:{item1:1,item2:1},user2:{item1:1}}\n",
    "dataset.user_item_matrix：M[user][item]=1,if user对item有行为 else M[user][item]=0\n",
    "dataset.userIndexer,dataset.itemIndexer. dataset索引器，作用通过字符ID 找到 矩阵索引\n",
    "result_pt ： 预测结果存入路径\n",
    "userCF_prediction_result_pt ：userCF预测结果保存变量名\n",
    "itemCF_prediction_result_pt ：itemCF预测结果保存变量名\n",
    "random_prediction_result_pt ：random预测结果保存变量名\n",
    "topn_prediction_result_pt ：topn预测结果保存变量名\n",
    "lfm_prediction_result_pt ：lfm预测结果保存变量名\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ml_latest_small.data import *\n",
    "from Collaborative_Filter.itemBased import ItemCFModel,ItemCF\n",
    "from Collaborative_Filter.userBased import UserCFModel,UserCF\n",
    "from Collaborative_Filter.lfmBased import lfmModel,lfm\n",
    "from Comparision.randomBased import RandomModel\n",
    "from Comparision.topnBased import topnModel\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import os\n",
    "from Evaluation.Evaluator import Evaluator\n",
    "import time\n",
    "def load_result(path):\n",
    "    pkl_file = open(path, 'rb')\n",
    "    prediction_dict = pickle.load(pkl_file)\n",
    "    # pprint.pprint(data)\n",
    "    pkl_file.close()\n",
    "    return prediction_dict\n",
    "\n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "#     popularity = 5\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "\n",
    "def itemCF_predict(train_dict,test_dict,ItemCF_model,K):\n",
    "    #recommendation for every user in train_Dataset\n",
    "    users = train_dict.keys()\n",
    "    user_nums = len(users)\n",
    "    i = 0\n",
    "    prediction_dict ={}\n",
    "    for user in users:\n",
    "        prediction_dict[user]=ItemCF_model.recommendation(user,config={'K':K,'N':10}).keys()\n",
    "    return prediction_dict\n",
    "\n",
    "start = time.time()\n",
    "train_dict = train.rdd.map(lambda x:(x['user'],(x['item'],x['rating'])))\\\n",
    "             .groupByKey().mapValues(dict).collectAsMap()\n",
    "test_dict = test.rdd.map(lambda x:(x['user'],(x['item'],x['rating'])))\\\n",
    "             .groupByKey().mapValues(dict).collectAsMap()\n",
    "itemCF_local = ItemCF()\n",
    "#triain ItemCF_model\n",
    "print \"训练基于物品协同过滤算法模型........\"\n",
    "ItemCF_model=itemCF_local.train(train_dict)\n",
    "for K in [5,10,20,40,60,80,100,120]:\n",
    "    prediction_dict = itemCF_predict(train_dict,test_dict,ItemCF_model,K) \n",
    "    evaluator = Evaluator(train_dict,test_dict,prediction_dict,N=10)\n",
    "    (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "    end = time.time()\n",
    "    print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "            %(K,precise*100,recall*100,coverage*100,popularity,end-start))\n",
    "    start = time.time()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  基于物品的协同过滤算法 spark实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "+----+----+------+---------+\n",
      "|user|item|rating|timestamp|\n",
      "+----+----+------+---------+\n",
      "|3093|   2|   3.0|969650197|\n",
      "|3095|2174|   4.0|970774487|\n",
      "|3097|2412|   1.0|969637398|\n",
      "|3100|3005|   3.0|969591786|\n",
      "|3101|1210|   4.0|969582345|\n",
      "+----+----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "total lines: 1000209\n",
      "model trainning spend: 14.8884420395 s\n",
      "NN:  5,  precise:29.59%,  recall:8.93%,  coverage:22.00%,  popularity:7.14, time: 463.315440893 s\n",
      "NN: 10,  precise:30.75%,  recall:9.28%,  coverage:19.07%,  popularity:7.23, time: 41.4911849499 s\n",
      "NN: 20,  precise:30.33%,  recall:9.15%,  coverage:16.95%,  popularity:7.30, time: 54.15498209 s\n",
      "NN: 40,  precise:29.57%,  recall:8.93%,  coverage:14.91%,  popularity:7.35, time: 77.9435040951 s\n",
      "NN: 60,  precise:29.08%,  recall:8.78%,  coverage:13.47%,  popularity:7.37, time: 97.563338995 s\n",
      "NN: 80,  precise:28.47%,  recall:8.59%,  coverage:12.98%,  popularity:7.37, time: 117.952799082 s\n",
      "NN:100,  precise:28.21%,  recall:8.52%,  coverage:12.52%,  popularity:7.36, time: 140.126652956 s\n",
      "NN:120,  precise:27.71%,  recall:8.37%,  coverage:11.90%,  popularity:7.36, time: 153.165653944 s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "#__author__:yanbin\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from operator import add,itemgetter\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pandas import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import math\n",
    "class ItemCFModel(object):  \n",
    "    def __init__(self,df,item_pair_sim=None,spark=None):\n",
    "        self.df = df\n",
    "        self.item_pair_sim = item_pair_sim\n",
    "\n",
    "    def itemSimilarity(self):\n",
    "        # RDD[(uid,(aid,score))] \n",
    "        user_item_score = self.df.rdd.map(lambda x:(x['user'],[x['item'],x['rating']]))\n",
    "        item_score_pair = user_item_score.join(user_item_score)\\\n",
    "                        .map(lambda x:((x[1][0][0],x[1][1][0]),(x[1][0][1],x[1][1][1])))\n",
    "        item_pair_ALL = item_score_pair.map(lambda f:(f[0], f[1][0] * f[1][1])).reduceByKey(add,300)\n",
    "        item_pair_XX_YY = item_pair_ALL.filter(lambda f:f[0][0] == f[0][1])\n",
    "        item_pair_XY = item_pair_ALL.filter(lambda f:f[0][0] != f[0][1])\n",
    "        #RDD[(aid1,score11 * score11 + score21 * score21)] \n",
    "        item_XX_YY = item_pair_XX_YY.map(lambda f:(f[0][0], f[1]))\n",
    "        #RDD(aid1,((aid1,aid2,XY),XX))\n",
    "        item_XY_XX = item_pair_XY.map(lambda f:(f[0][0], (f[0][0], f[0][1], f[1]))).join(item_XX_YY) \n",
    "        #RDD[(aid2,((aid1,aid2,\n",
    "        #           score11 * score12 + score21 * score22,score11 * score11 + score21 * score21),\n",
    "        #           score12 * score12 + score22 * score22))] \n",
    "        item_XY_XX_YY = item_XY_XX.map(lambda f:(f[1][0][1],(f[1][0][0],f[1][0][1],f[1][0][2],f[1][1]))).join(item_XX_YY)  \n",
    "        # item_XY_XX_YY中的(aid1,aid2,XY,XX,YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22,score11 * score11 + score21 * score21,score12 * score12 + score22 * score22)]       \n",
    "        item_pair_XY_XX_YY = item_XY_XX_YY.map(lambda f:(f[1][0][0], f[1][0][1], f[1][0][2], f[1][0][3], f[1][1]))  \n",
    "        # item_pair_XY_XX_YY为(aid1,aid2,XY / math.sqrt(XX * YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22 / math.sqrt((score11 * score11 + score21 * score21)*(score12 * score12 + score22 * score22))] \n",
    "        item_pair_sim = item_pair_XY_XX_YY.map(lambda f :(f[0], (f[1], f[2] / math.sqrt(f[3] * f[4]))))  \n",
    "        return item_pair_sim\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        item_pair_sim = self.itemSimilarity()\n",
    "        item_pair_sim.cache()  \n",
    "        self.item_pair_sim=item_pair_sim\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def recommend(df,item_pair_sim,NN=100,topN =10,Normalization=False):\n",
    "    def itemNN(item_pair_sim,NN=100,Normalization=False):\n",
    "        item_sim = item_pair_sim.filter(lambda f:f[1][1]>0.05)\\\n",
    "                            .groupByKey()\\\n",
    "                            .mapValues(list)\n",
    "        if Normalization:\n",
    "            def norm(x):\n",
    "                m =  __builtin__.max([i[1] for i in x])\n",
    "                l = []\n",
    "                for i in x:\n",
    "                    l.append((i[0],i[1]/m))\n",
    "                return l\n",
    "            item_sim = item_sim.mapValues(lambda x:norm(x))\n",
    "        item_simNN = item_sim.mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:NN])\\\n",
    "                            .collectAsMap()\n",
    "        return item_simNN\n",
    "    \n",
    "    def getOrElse(f,item_sim_bd):\n",
    "        items_sim = item_sim_bd.value.get(f[0][1]) \n",
    "        if items_sim is None:\n",
    "            items_sim = [(\"0\", 0.0)]\n",
    "        for w in items_sim:\n",
    "            yield ((f[0][0],w[0]),w[1]*f[1])\n",
    "            \n",
    "    user_item_score = df.rdd.map(lambda x:((x['user'],x['item']),x['rating']))\n",
    "    item_sim_bd = sc.broadcast(itemNN(item_pair_sim,NN=NN,Normalization=Normalization))\n",
    "#     /* \n",
    "#      * 提取item_sim_user_score为((user,item2),sim * score) \n",
    "#      * RDD[(user,item2),sim * score] \n",
    "#      */  \n",
    "\n",
    "    user_item_simscore = user_item_score.flatMap(lambda f:getOrElse(f,item_sim_bd))\\\n",
    "                                        .filter(lambda f:f[1]> 0.03)  \n",
    "#       /*\n",
    "#      * 聚合user_item_simscore为 (user,（item2,sim1 * score1 + sim2 * score2）)\n",
    "#      * 假设user观看过两个item,评分分别为score1和score2，item2是与user观看过的两个item相似的item,相似度分别为sim1，sim2 \n",
    "#      * RDD[(user,item2),sim1 * score1 + sim2 * score2）)] \n",
    "#      */  \n",
    "    user_item_rank = user_item_simscore.reduceByKey(add,1000)  \n",
    "\n",
    "#     /* \n",
    "#      * 过滤用户已看过的item,并对user_item_rank基于user聚合 \n",
    "#      * RDD[(user,CompactBuffer((item2,rank2）,(item3,rank3)...))] \n",
    "#      */  \n",
    "    user_items_ranks = user_item_rank.subtractByKey(user_item_score)\\\n",
    "                                     .map(lambda f:(f[0][0], (f[0][1], f[1])))\\\n",
    "                                     .groupByKey()  \n",
    "#     /* \n",
    "#      * 对user_items_ranks基于rank降序排序，并提取topN,其中包括用户已观看过的item \n",
    "#      * RDD[(user,ArrayBuffer((item2,rank2）,...,(itemN,rankN)))] \n",
    "#      */  \n",
    "    user_items_ranks_desc = user_items_ranks.mapValues(list)\\\n",
    "                            .mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:topN])\n",
    "    return user_items_ranks_desc\n",
    "\n",
    "from Evaluator import Evaluator\n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    import os\n",
    "    PYSPARK_PYTHON = \"/usr/bin/python2.7\"\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "    conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "    conf.set('spark.yarn.dist.files',\n",
    "            'file:/root/hadoop-2.6/spark/python/lib/pyspark.zip,file:/root/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip')\n",
    "    conf.set('spark.executor.cores','30')\n",
    "    conf.set('spark.executor.memory','95g')\n",
    "    conf.set('spark.executor.instances','8')\n",
    "    conf.set('spark.sql.shuffle.partitions','400')\n",
    "    conf.set('spark.default.parallelism','200')\n",
    "#     conf.set(\"spark.shuffle.file.buffer\",\"128k\").set(\"spark.reducer.maxSizeInFlight\",\"96M\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    sc=spark.sparkContext \n",
    "    sc.setLogLevel('WARN')\n",
    "    start = time.time()\n",
    "    inputPath = 'data/ml-1m/ratings.dat'\n",
    "    schema = StructType([\n",
    "            StructField(\"user\", StringType(), True),\n",
    "            StructField(\"item\", StringType(), True),\n",
    "            StructField(\"rating\", DoubleType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)])\n",
    "    ratingRdd = sc.textFile(inputPath).map(lambda line:line.split(\"::\"))\\\n",
    "                    .map(lambda x:(x[0],x[1],float(x[2]),long(x[3])))\n",
    "    ratingDf = spark.createDataFrame(data=ratingRdd,schema=schema)\n",
    "    #     ratingDf,_ = ratingDf.randomSplit([1.0,9.0],seed=40)\n",
    "    ratingDf = ratingDf.repartition(300)\n",
    "    ratingDf.printSchema()\n",
    "    ratingDf.show(5)\n",
    "    n = ratingDf.count()\n",
    "#     ratingDf = ratingDf.withColumn('score',col('rating')*0+1).select('user','item','score')    \n",
    "    print 'total lines: %s' %n\n",
    "    train,test = ratingDf.randomSplit([4.0,1.0],seed=40)\n",
    "    train.cache()\n",
    "    test.cache()\n",
    "    itemCF = ItemCFModel(df=train,spark=spark)\n",
    "    itemCF.train()\n",
    "    end = time.time()\n",
    "    print 'model trainning spend: %s s' %(end-start)\n",
    "    start = time.time()\n",
    "    for NN in [5,10,20,40,60,80,100,120]:\n",
    "        recTopN = recommend(train,itemCF.item_pair_sim,NN=NN,topN=10,Normalization=False)\n",
    "        pre = spark.createDataFrame(data=recTopN.flatMapValues(lambda x:x).map(lambda x:(x[0],x[1][0],x[1][1])),\n",
    "                                    schema=['user','item','rating'])\n",
    "        evaluator = Evaluator(train,test,pre)\n",
    "        (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "        end = time.time()\n",
    "        print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "                %(NN,precise*100,recall*100,coverage*100,popularity,end-start))\n",
    "        pre.write.parquet(partitionBy='user',path='data/ml-1m/result/itemCF/itcf_'+str(NN),mode='ignore')\n",
    "        start = time.time()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "+----+----+------+---------+\n",
      "|user|item|rating|timestamp|\n",
      "+----+----+------+---------+\n",
      "|3093|   2|   3.0|969650197|\n",
      "|3095|2174|   4.0|970774487|\n",
      "|3097|2412|   1.0|969637398|\n",
      "|3100|3005|   3.0|969591786|\n",
      "|3101|1210|   4.0|969582345|\n",
      "+----+----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "total lines: 1000209\n",
      "NN:  5,  precise:30.00%,  recall:9.06%,  coverage:28.60%,  popularity:7.03, time: 203.805400133 s\n",
      "NN: 10,  precise:31.37%,  recall:9.47%,  coverage:23.85%,  popularity:7.14, time: 28.1104741096 s\n",
      "NN: 20,  precise:31.40%,  recall:9.48%,  coverage:19.83%,  popularity:7.23, time: 36.9957470894 s\n",
      "NN: 40,  precise:30.81%,  recall:9.30%,  coverage:17.68%,  popularity:7.29, time: 46.629994154 s\n",
      "NN: 60,  precise:30.21%,  recall:9.12%,  coverage:15.94%,  popularity:7.32, time: 58.6860690117 s\n",
      "NN: 80,  precise:29.48%,  recall:8.90%,  coverage:15.15%,  popularity:7.32, time: 69.0329020023 s\n",
      "NN:100,  precise:29.12%,  recall:8.79%,  coverage:14.37%,  popularity:7.32, time: 77.2784481049 s\n",
      "NN:120,  precise:28.47%,  recall:8.59%,  coverage:13.93%,  popularity:7.32, time: 90.3741400242 s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "#__author__:yanbin\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from operator import add,itemgetter\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pandas import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import math\n",
    "class ItemCFModel(object):  \n",
    "    def __init__(self,df,item_pair_sim=None,spark=None):\n",
    "        self.df = df\n",
    "        self.item_pair_sim = item_pair_sim\n",
    "\n",
    "    def itemSimilarity(self):\n",
    "        # RDD[(uid,(aid,score))] \n",
    "        user_item_score = self.df.rdd.map(lambda x:(x['user'],[x['item'],x['rating']]))\n",
    "        item_score_pair = user_item_score.join(user_item_score)\\\n",
    "                        .map(lambda x:((x[1][0][0],x[1][1][0]),(x[1][0][1],x[1][1][1])))\n",
    "        item_pair_ALL = item_score_pair.map(lambda f:(f[0], f[1][0] * f[1][1])).reduceByKey(add,300)\n",
    "        item_pair_XX_YY = item_pair_ALL.filter(lambda f:f[0][0] == f[0][1])\n",
    "        item_pair_XY = item_pair_ALL.filter(lambda f:f[0][0] != f[0][1])\n",
    "        #RDD[(aid1,score11 * score11 + score21 * score21)] \n",
    "        item_XX_YY = item_pair_XX_YY.map(lambda f:(f[0][0], f[1]))\n",
    "        #RDD(aid1,((aid1,aid2,XY),XX))\n",
    "        item_XY_XX = item_pair_XY.map(lambda f:(f[0][0], (f[0][0], f[0][1], f[1]))).join(item_XX_YY) \n",
    "        #RDD[(aid2,((aid1,aid2,\n",
    "        #           score11 * score12 + score21 * score22,score11 * score11 + score21 * score21),\n",
    "        #           score12 * score12 + score22 * score22))] \n",
    "        item_XY_XX_YY = item_XY_XX.map(lambda f:(f[1][0][1],(f[1][0][0],f[1][0][1],f[1][0][2],f[1][1]))).join(item_XX_YY)  \n",
    "        # item_XY_XX_YY中的(aid1,aid2,XY,XX,YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22,score11 * score11 + score21 * score21,score12 * score12 + score22 * score22)]       \n",
    "        item_pair_XY_XX_YY = item_XY_XX_YY.map(lambda f:(f[1][0][0], f[1][0][1], f[1][0][2], f[1][0][3], f[1][1]))  \n",
    "        # item_pair_XY_XX_YY为(aid1,aid2,XY / math.sqrt(XX * YY)) \n",
    "        # RDD[(aid1,aid2,\n",
    "        # score11 * score12 + score21 * score22 / math.sqrt((score11 * score11 + score21 * score21)*(score12 * score12 + score22 * score22))] \n",
    "        item_pair_sim = item_pair_XY_XX_YY.map(lambda f :(f[0], (f[1], f[2] / math.sqrt(f[3] * f[4]))))  \n",
    "        return item_pair_sim\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        item_pair_sim = self.itemSimilarity()\n",
    "        item_pair_sim.cache()  \n",
    "        self.item_pair_sim=item_pair_sim\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def recommend(df,item_pair_sim,NN=100,topN =10,Normalization=False):\n",
    "    def itemNN(item_pair_sim,NN=100,Normalization=False):\n",
    "        item_sim = item_pair_sim.filter(lambda f:f[1][1]>0.05)\\\n",
    "                            .groupByKey()\\\n",
    "                            .mapValues(list)\n",
    "        if Normalization:\n",
    "            def norm(x):\n",
    "                m =  __builtin__.max([i[1] for i in x])\n",
    "                l = []\n",
    "                for i in x:\n",
    "                    l.append((i[0],i[1]/m))\n",
    "                return l\n",
    "            item_sim = item_sim.mapValues(lambda x:norm(x))\n",
    "        item_simNN = item_sim.mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:NN])\\\n",
    "                            .collectAsMap()\n",
    "        return item_simNN\n",
    "    \n",
    "    def getOrElse(f,item_sim_bd):\n",
    "        items_sim = item_sim_bd.value.get(f[0][1]) \n",
    "        if items_sim is None:\n",
    "            items_sim = [(\"0\", 0.0)]\n",
    "        for w in items_sim:\n",
    "            yield ((f[0][0],w[0]),w[1]*f[1])\n",
    "            \n",
    "    user_item_score = df.rdd.map(lambda x:((x['user'],x['item']),x['rating']))\n",
    "    item_sim_bd = sc.broadcast(itemNN(item_pair_sim,NN=NN,Normalization=Normalization))\n",
    "#     /* \n",
    "#      * 提取item_sim_user_score为((user,item2),sim * score) \n",
    "#      * RDD[(user,item2),sim * score] \n",
    "#      */  \n",
    "\n",
    "    user_item_simscore = user_item_score.flatMap(lambda f:getOrElse(f,item_sim_bd))\\\n",
    "                                        .filter(lambda f:f[1]> 0.03)  \n",
    "#       /*\n",
    "#      * 聚合user_item_simscore为 (user,（item2,sim1 * score1 + sim2 * score2）)\n",
    "#      * 假设user观看过两个item,评分分别为score1和score2，item2是与user观看过的两个item相似的item,相似度分别为sim1，sim2 \n",
    "#      * RDD[(user,item2),sim1 * score1 + sim2 * score2）)] \n",
    "#      */  \n",
    "    user_item_rank = user_item_simscore.reduceByKey(add,1000)  \n",
    "\n",
    "#     /* \n",
    "#      * 过滤用户已看过的item,并对user_item_rank基于user聚合 \n",
    "#      * RDD[(user,CompactBuffer((item2,rank2）,(item3,rank3)...))] \n",
    "#      */  \n",
    "    user_items_ranks = user_item_rank.subtractByKey(user_item_score)\\\n",
    "                                     .map(lambda f:(f[0][0], (f[0][1], f[1])))\\\n",
    "                                     .groupByKey()  \n",
    "#     /* \n",
    "#      * 对user_items_ranks基于rank降序排序，并提取topN,其中包括用户已观看过的item \n",
    "#      * RDD[(user,ArrayBuffer((item2,rank2）,...,(itemN,rankN)))] \n",
    "#      */  \n",
    "    user_items_ranks_desc = user_items_ranks.mapValues(list)\\\n",
    "                            .mapValues(lambda x:sorted(x,key=itemgetter(1),reverse=True)[:topN])\n",
    "    return user_items_ranks_desc\n",
    "\n",
    "from Evaluator import Evaluator\n",
    "def evaluate(evaluator):\n",
    "    precise = evaluator.precision()\n",
    "    coverage = evaluator.coverage()\n",
    "    popularity = evaluator.popularity()\n",
    "    recall = evaluator.recall()\n",
    "    return precise,recall,coverage,popularity\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    import os\n",
    "    PYSPARK_PYTHON = \"/usr/bin/python2.7\"\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "    conf = SparkConf().setAppName(\"itemCF\").setMaster(\"yarn\")\n",
    "    conf.set('spark.yarn.dist.files',\n",
    "            'file:/root/hadoop-2.6/spark/python/lib/pyspark.zip,file:/root/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip')\n",
    "    conf.set('spark.executor.cores','30')\n",
    "    conf.set('spark.executor.memory','90g')\n",
    "    conf.set('spark.executor.instances','4')\n",
    "    conf.set('spark.sql.shuffle.partitions','400')\n",
    "    conf.set('spark.default.parallelism','200')\n",
    "#     conf.set(\"spark.shuffle.file.buffer\",\"128k\").set(\"spark.reducer.maxSizeInFlight\",\"96M\")\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    sc=spark.sparkContext \n",
    "    sc.setLogLevel('WARN')\n",
    "    start = time.time()\n",
    "    inputPath = 'data/ml-1m/ratings.dat'\n",
    "    schema = StructType([\n",
    "            StructField(\"user\", StringType(), True),\n",
    "            StructField(\"item\", StringType(), True),\n",
    "            StructField(\"rating\", DoubleType(), True),\n",
    "            StructField(\"timestamp\", LongType(), True)])\n",
    "    ratingRdd = sc.textFile(inputPath).map(lambda line:line.split(\"::\"))\\\n",
    "                    .map(lambda x:(x[0],x[1],float(x[2]),long(x[3])))\n",
    "    ratingDf = spark.createDataFrame(data=ratingRdd,schema=schema)\n",
    "    #     ratingDf,_ = ratingDf.randomSplit([1.0,9.0],seed=40)\n",
    "    ratingDf = ratingDf.repartition(300)\n",
    "    ratingDf.printSchema()\n",
    "    ratingDf.show(5)\n",
    "    n = ratingDf.count()\n",
    "#     ratingDf = ratingDf.withColumn('score',col('rating')*0+1).select('user','item','score')    \n",
    "    print 'total lines: %s' %n\n",
    "    train,test = ratingDf.randomSplit([4.0,1.0],seed=40)\n",
    "    train.cache()\n",
    "    test.cache()\n",
    "    itemCF = ItemCFModel(df=train,spark=spark)\n",
    "    itemCF.train()\n",
    "    for NN in [5,10,20,40,60,80,100,120]:\n",
    "#     for NN in [100]:\n",
    "        recTopN = recommend(train,itemCF.item_pair_sim,NN=NN,topN=10,Normalization=True)\n",
    "        pre = spark.createDataFrame(data=recTopN.flatMapValues(lambda x:x).map(lambda x:(x[0],x[1][0],x[1][1])),\n",
    "                                    schema=['user','item','rating'])\n",
    "        evaluator = Evaluator(train,test,pre)\n",
    "        (precise,recall,coverage,popularity) = evaluate(evaluator)\n",
    "        end = time.time()\n",
    "        print ('NN:%3d,  precise:%2.2f%%,  recall:%2.2f%%,  coverage:%2.2f%%,  popularity:%2.2f, time: %s s'\n",
    "                %(NN,precise*100,recall*100,coverage*100,popularity,end-start))\n",
    "        pre.write.parquet(partitionBy='user',path='data/ml-1m/result/itemCF/itcf_'+str(NN),mode='ignore')\n",
    "        start = time.time()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|       user_count|        item_count|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|             6040|              3706|\n",
      "|   mean|165.5975165562914|269.88909875876953|\n",
      "| stddev|192.7470290697777| 384.0478375720254|\n",
      "|    min|               20|                 1|\n",
      "|    max|             2314|              3428|\n",
      "+-------+-----------------+------------------+\n",
      "\n",
      "稀疏度：4.468363%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26eea7fe10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26f665ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD8CAYAAACPWyg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26eea67890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !! hadoop fs -ls data/ml-1m/result/itemCF\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "n = ratingDf.count()\n",
    "ui_count  = ratingDf.agg(count('*').alias('ui'))\n",
    "user_count = ratingDf.groupBy('user').agg(count('*').alias('user_count'))\n",
    "item_count = ratingDf.groupBy('item').agg(count('*').alias('item_count'))\n",
    "count_union = user_count.describe('user_count')\\\n",
    "          .join(item_count.describe('item_count'),on='summary',how='inner')\n",
    "count_union.show()\n",
    "print '稀疏度：%f%%' % (n/(6040*3706*1.0)*100)\n",
    "user_count.toPandas().plot.hist(bins=30)\n",
    "item_count.toPandas().plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
