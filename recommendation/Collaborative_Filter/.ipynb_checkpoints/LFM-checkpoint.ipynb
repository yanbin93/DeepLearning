{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUserPositiveItem(frame, userID):\n",
    "    '''\n",
    "    获取用户正反馈物品：用户评分过的物品\n",
    "    :param frame: ratings数据\n",
    "    :param userID: 用户ID\n",
    "    :return: 正反馈物品\n",
    "    '''\n",
    "    series = frame[frame['UserID'] == userID]['MovieID']\n",
    "    positiveItemList = list(series.values)\n",
    "    return positiveItemList\n",
    "\n",
    "def getUserNegativeItem(frame, userID):\n",
    "    '''\n",
    "    获取用户负反馈物品：热门但是用户没有进行过评分 与正反馈数量相等\n",
    "    :param frame: ratings数据\n",
    "    :param userID:用户ID\n",
    "    :return: 负反馈物品\n",
    "    '''\n",
    "    userItemlist = list(set(frame[frame['UserID'] == userID]['MovieID']))                       \n",
    "    #用户评分过的物品\n",
    "    otherItemList = [item for item in set(frame['MovieID'].values) if item not in userItemlist] \n",
    "    #用户没有评分的物品\n",
    "    itemCount = [len(frame[frame['MovieID'] == item]['UserID']) for item in otherItemList]      \n",
    "    #物品热门程度\n",
    "    series = pd.Series(itemCount, index=otherItemList)\n",
    "    series = series.sort_values(ascending=False)[:len(userItemlist)]                            \n",
    "    #获取正反馈物品数量的负反馈物品\n",
    "    negativeItemList = list(series.index)\n",
    "    return negativeItemList\n",
    "def initPara(userID, itemID, classCount):\n",
    "    '''\n",
    "    初始化参数q,p矩阵, 随机\n",
    "    :param userCount:用户ID\n",
    "    :param itemCount:物品ID\n",
    "    :param classCount: 隐类数量\n",
    "    :return: 参数p,q\n",
    "    '''\n",
    "    arrayp = np.random.rand(len(userID), classCount)\n",
    "    arrayq = np.random.rand(classCount, len(itemID))\n",
    "    p = pd.DataFrame(arrayp, columns=range(0,classCount), index=userID)\n",
    "    q = pd.DataFrame(arrayq, columns=itemID, index=range(0,classCount))\n",
    "    return p,q\n",
    "def lfmPredict(p, q, userID, itemID):\n",
    "    '''\n",
    "    利用参数p,q预测目标用户对目标物品的兴趣度\n",
    "    :param p: 用户兴趣和隐类的关系\n",
    "    :param q: 隐类和物品的关系\n",
    "    :param userID: 目标用户\n",
    "    :param itemID: 目标物品\n",
    "    :return: 预测兴趣度\n",
    "    '''\n",
    "    p = np.mat(p.ix[userID].values)\n",
    "    q = np.mat(q[itemID].values).T\n",
    "    r = (p * q).sum()\n",
    "    r = sigmod(r)\n",
    "    return r\n",
    "\n",
    "def sigmod(x):\n",
    "    '''\n",
    "    单位阶跃函数,将兴趣度限定在[0,1]范围内\n",
    "    :param x: 兴趣度\n",
    "    :return: 兴趣度\n",
    "    '''\n",
    "    y = 1.0/(1+exp(-x))\n",
    "    return y\n",
    "\n",
    "def LFM(user_items, F, N, alpha, lambda):  \n",
    "    #初始化P,Q矩阵  \n",
    "    [P, Q] = InitModel(user_items, F)  \n",
    "    #开始迭代  \n",
    "    For step in range(0, N):  \n",
    "        #从数据集中依次取出user以及该user喜欢的iterms集  \n",
    "        for user, items in user_item.iterms():  \n",
    "            #随机抽样，为user抽取与items数量相当的负样本，并将正负样本合并，用于优化计算  \n",
    "            samples = RandSelectNegativeSamples(items)  \n",
    "            #依次获取item和user对该item的兴趣度  \n",
    "            for item, rui in samples.items():  \n",
    "                #根据当前参数计算误差  \n",
    "                eui = eui - Predict(user, item)  \n",
    "                #优化参数  \n",
    "                for f in range(0, F):  \n",
    "                    P[user][f] += alpha * (eui * Q[f][item] - lambda * P[user][f])  \n",
    "                    Q[f][item] += alpha * (eui * P[user][f] - lambda * Q[f][item])  \n",
    "        #每次迭代完后，都要降低学习速率。一开始的时候由于离最优值相差甚远，因此快速下降；  \n",
    "        #当优化到一定程度后，就需要放慢学习速率，慢慢的接近最优值。  \n",
    "        alpha *= 0.9  \n",
    "        \n",
    "def recommend(frame, userID, p, q, TopN=10):\n",
    "    '''\n",
    "    推荐TopN个物品给目标用户\n",
    "    :param frame: 源数据\n",
    "    :param userID: 目标用户\n",
    "    :param p: 用户兴趣和隐类的关系\n",
    "    :param q: 隐类和物品的关系\n",
    "    :param TopN: 推荐数量\n",
    "    :return: 推荐物品\n",
    "    '''\n",
    "    userItemlist = list(set(frame[frame['UserID'] == userID]['MovieID']))\n",
    "    otherItemList = [item for item in set(frame['MovieID'].values) if item not in userItemlist]\n",
    "    predictList = [lfmPredict(p, q, userID, itemID) for itemID in otherItemList]\n",
    "    series = pd.Series(predictList, index=otherItemList)\n",
    "    series = series.sort_values(ascending=False)[:TopN]\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.809217334785\n",
      "b 1.01209277051\n",
      "c 0.894069356814\n",
      "d 0.694955681096\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "__author__ = \"orisun\"\n",
    " \n",
    "import random\n",
    "import math\n",
    " \n",
    "class LFM(object):\n",
    " \n",
    "    def __init__(self, rating_data, F, alpha=0.1, lmbd=0.1, max_iter=500):\n",
    "        '''rating_data是list<(user,list<(position,rate)>)>类型\n",
    "        '''\n",
    "        self.F = F\n",
    "        self.P = dict()  # R=PQ^T，代码中的Q相当于博客中Q的转置\n",
    "        self.Q = dict()\n",
    "        self.alpha = alpha\n",
    "        self.lmbd = lmbd\n",
    "        self.max_iter = max_iter\n",
    "        self.rating_data = rating_data\n",
    " \n",
    "        '''随机初始化矩阵P和Q'''\n",
    "        for user, rates in self.rating_data:\n",
    "            self.P[user] = [random.random() / math.sqrt(self.F)\n",
    "                            for x in xrange(self.F)]\n",
    "            for item, _ in rates:\n",
    "                if item not in self.Q:\n",
    "                    self.Q[item] = [random.random() / math.sqrt(self.F)\n",
    "                                    for x in xrange(self.F)]\n",
    " \n",
    "    def train(self):\n",
    "        '''随机梯度下降法训练参数P和Q\n",
    "        '''\n",
    "        for step in xrange(self.max_iter):\n",
    "            for user, rates in self.rating_data:\n",
    "                for item, rui in rates:\n",
    "                    hat_rui = self.predict(user, item)\n",
    "                    err_ui = rui - hat_rui\n",
    "                    for f in xrange(self.F):\n",
    "                        self.P[user][f] += self.alpha * (err_ui * self.Q[item][f] - self.lmbd * self.P[user][f])\n",
    "                        self.Q[item][f] += self.alpha * (err_ui * self.P[user][f] - self.lmbd * self.Q[item][f])\n",
    "            self.alpha *= 0.9  # 每次迭代步长要逐步缩小\n",
    " \n",
    "    def predict(self, user, item):\n",
    "        '''预测用户user对物品item的评分\n",
    "        '''\n",
    "        return sum(self.P[user][f] * self.Q[item][f] for f in xrange(self.F))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''用户有A B C，物品有a b c d'''\n",
    "    rating_data = list()\n",
    "    rate_A = [('a', 1.0), ('b', 1.0)]\n",
    "    rating_data.append(('A', rate_A))\n",
    "    rate_B = [('b', 1.0), ('c', 1.0)]\n",
    "    rating_data.append(('B', rate_B))\n",
    "    rate_C = [('c', 1.0), ('d', 1.0)]\n",
    "    rating_data.append(('C', rate_C))\n",
    "    #sample movie examples\n",
    "#     rate_A = [('Harry Potter',1.0),('Avatar',1.0),('LOTR 3',1.0),('Gladiator',0.0),('Titanic',0.0),('Glitter',0.0)]\n",
    "#     rating_data.append(('Alice',rate_A))\n",
    "#     rate_B = [('Harry Potter',1.0),('Avatar',0.0),('LOTR 3',1.0),('Gladiator',0.0),('Titanic',0.0),('Glitter',0.0)]\n",
    "#     rating_data.append(('Bob',rate_B))#SF/fantasy fan\n",
    "#     rate_C = [('Harry Potter',1.0),('Avatar',1.0),('LOTR 3',1.0),('Gladiator',0.0),('Titanic',0.0),('Glitter',0.0)]\n",
    "#     rating_data.append(('Carol',rate_C))#Big SF/fantasy fan\n",
    "#     rate_D = [('Harry Potter',0.0),('Avatar',0.0),('LOTR 3',1.0),('Gladiator',1.0),('Titanic',1.0),('Glitter',0.0)]\n",
    "#     rating_data.append(('David',rate_D))#Big Oscar winners fan\n",
    "#     rate_E = [('Harry Potter',0.0),('Avatar',0.0),('LOTR 3',1.0),('Gladiator',1.0),('Titanic',1.0),('Glitter',0.0)]\n",
    "#     rating_data.append(('Eric',rate_E))#Oscar winners fan,except for Titanic\n",
    "#     rate_F = [('Harry Potter',0.0),('Avatar',0.0),('LOTR 3',1.0),('Gladiator',1.0),('Titanic',1.0),('Glitter',0.0)]\n",
    "#     rating_data.append(('Fred',rate_F))#Oscar winners fan,except for Titanic\n",
    "#     rate_G = [('Harry Potter',0), ('Avatar',0),('LOTR 3',0), ('Gladiator',1.0), ('Titanic',1.0), ('Glitter',0)] \n",
    "    lfm = LFM(rating_data, 2)\n",
    "    lfm.train()\n",
    "    for item in ['a', 'b', 'c', 'd']:\n",
    "        print item, lfm.predict('C', item)      #计算用户A对各个物品的喜好程度\n",
    "#     for item in ['Harry Potter','Avatar','LOTR 3','Gladiator','Titanic','Glitter']:\n",
    "#         print item, lfm.predict('Alice',item)\n",
    "#     for u in sorted(lfm.P.keys()):\n",
    "#         print u,lfm.P[u]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, error is 3.012735\n",
      "iteration 1, error is 2.995830\n",
      "iteration 2, error is 2.975186\n",
      "iteration 3, error is 2.989292\n",
      "iteration 4, error is 2.928172\n",
      "iteration 5, error is 3.019785\n",
      "iteration 6, error is 2.914794\n",
      "iteration 7, error is 2.859600\n",
      "iteration 8, error is 2.964442\n",
      "iteration 9, error is 2.965668\n",
      "iteration 10, error is 2.964141\n",
      "iteration 11, error is 2.921345\n",
      "iteration 12, error is 2.924412\n",
      "iteration 13, error is 2.888505\n",
      "iteration 14, error is 3.043583\n",
      "iteration 15, error is 2.970955\n",
      "iteration 16, error is 2.858787\n",
      "iteration 17, error is 2.828750\n",
      "iteration 18, error is 2.870662\n",
      "iteration 19, error is 2.829418\n",
      "iteration 20, error is 2.698671\n",
      "iteration 21, error is 2.769905\n",
      "iteration 22, error is 3.027798\n",
      "iteration 23, error is 2.585815\n",
      "iteration 24, error is 2.997120\n",
      "iteration 25, error is 2.858570\n",
      "iteration 26, error is 2.996520\n",
      "iteration 27, error is 3.065192\n",
      "iteration 28, error is 2.617551\n",
      "iteration 29, error is 2.587556\n",
      "iteration 30, error is 2.580958\n",
      "iteration 31, error is 3.012777\n",
      "iteration 32, error is 2.852464\n",
      "iteration 33, error is 2.597160\n",
      "iteration 34, error is 2.845077\n",
      "iteration 35, error is 2.717239\n",
      "iteration 36, error is 2.532924\n",
      "iteration 37, error is 2.492448\n",
      "iteration 38, error is 2.583025\n",
      "iteration 39, error is 2.873596\n",
      "iteration 40, error is 2.602037\n",
      "iteration 41, error is 2.604790\n",
      "iteration 42, error is 2.435639\n",
      "iteration 43, error is 2.620290\n",
      "iteration 44, error is 2.678268\n",
      "iteration 45, error is 2.568620\n",
      "iteration 46, error is 2.441859\n",
      "iteration 47, error is 2.612603\n",
      "iteration 48, error is 2.911099\n",
      "iteration 49, error is 3.128149\n",
      "iteration 50, error is 2.624693\n",
      "iteration 51, error is 2.660570\n",
      "iteration 52, error is 2.414943\n",
      "iteration 53, error is 2.606247\n",
      "iteration 54, error is 2.739627\n",
      "iteration 55, error is 2.711299\n",
      "iteration 56, error is 2.399861\n",
      "iteration 57, error is 2.591995\n",
      "iteration 58, error is 2.428640\n",
      "iteration 59, error is 2.419561\n",
      "iteration 60, error is 2.484407\n",
      "iteration 61, error is 2.512800\n",
      "iteration 62, error is 2.544956\n",
      "iteration 63, error is 2.609994\n",
      "iteration 64, error is 2.956084\n",
      "iteration 65, error is 2.471473\n",
      "iteration 66, error is 2.614000\n",
      "iteration 67, error is 2.612634\n",
      "iteration 68, error is 2.590312\n",
      "iteration 69, error is 2.959556\n",
      "iteration 70, error is 2.292080\n",
      "iteration 71, error is 3.050153\n",
      "iteration 72, error is 2.716601\n",
      "iteration 73, error is 2.503933\n",
      "iteration 74, error is 2.635956\n",
      "iteration 75, error is 2.630394\n",
      "iteration 76, error is 2.672040\n",
      "iteration 77, error is 3.271511\n",
      "iteration 78, error is 2.656217\n",
      "iteration 79, error is 2.452386\n",
      "iteration 80, error is 2.398360\n",
      "iteration 81, error is 2.851351\n",
      "iteration 82, error is 2.988613\n",
      "iteration 83, error is 2.492241\n",
      "iteration 84, error is 2.267939\n",
      "iteration 85, error is 2.452049\n",
      "iteration 86, error is 2.359325\n",
      "iteration 87, error is 2.310296\n",
      "iteration 88, error is 2.856751\n",
      "iteration 89, error is 2.491427\n",
      "iteration 90, error is 2.721839\n",
      "iteration 91, error is 2.360697\n",
      "iteration 92, error is 2.419345\n",
      "iteration 93, error is 2.757043\n",
      "iteration 94, error is 2.467619\n",
      "iteration 95, error is 2.323939\n",
      "iteration 96, error is 2.419805\n",
      "iteration 97, error is 2.539687\n",
      "iteration 98, error is 2.725921\n",
      "iteration 99, error is 2.619802\n",
      "iteration 100, error is 2.785977\n",
      "iteration 101, error is 2.656793\n",
      "iteration 102, error is 3.194736\n",
      "iteration 103, error is 3.090998\n",
      "iteration 104, error is 2.687647\n",
      "iteration 105, error is 2.498897\n",
      "iteration 106, error is 2.431290\n",
      "iteration 107, error is 2.618920\n",
      "iteration 108, error is 2.594097\n",
      "iteration 109, error is 2.304058\n",
      "iteration 110, error is 2.495897\n",
      "iteration 111, error is 2.147781\n",
      "iteration 112, error is 3.160283\n",
      "iteration 113, error is 2.518574\n",
      "iteration 114, error is 2.881804\n",
      "iteration 115, error is 2.496721\n",
      "iteration 116, error is 2.368961\n",
      "iteration 117, error is 2.422283\n",
      "iteration 118, error is 2.407340\n",
      "iteration 119, error is 2.761680\n",
      "iteration 120, error is 2.513805\n",
      "iteration 121, error is 2.928098\n",
      "iteration 122, error is 2.481201\n",
      "iteration 123, error is 2.407308\n",
      "iteration 124, error is 2.510377\n",
      "iteration 125, error is 2.229341\n",
      "iteration 126, error is 2.599043\n",
      "iteration 127, error is 2.520600\n",
      "iteration 128, error is 2.113809\n",
      "iteration 129, error is 2.391369\n",
      "iteration 130, error is 2.291578\n",
      "iteration 131, error is 2.512915\n",
      "iteration 132, error is 2.185485\n",
      "iteration 133, error is 2.592137\n",
      "iteration 134, error is 2.541374\n",
      "iteration 135, error is 2.234582\n",
      "iteration 136, error is 2.606071\n",
      "iteration 137, error is 2.445558\n",
      "iteration 138, error is 2.906716\n",
      "iteration 139, error is 2.757645\n",
      "iteration 140, error is 2.593907\n",
      "iteration 141, error is 2.472397\n",
      "iteration 142, error is 2.698307\n",
      "iteration 143, error is 2.283002\n",
      "iteration 144, error is 1.942960\n",
      "iteration 145, error is 3.146518\n",
      "iteration 146, error is 3.104342\n",
      "iteration 147, error is 2.052718\n",
      "iteration 148, error is 3.002799\n",
      "iteration 149, error is 2.704358\n",
      "iteration 150, error is 2.377378\n",
      "iteration 151, error is 2.426625\n",
      "iteration 152, error is 2.385090\n",
      "iteration 153, error is 3.029333\n",
      "iteration 154, error is 2.013042\n",
      "iteration 155, error is 3.090979\n",
      "iteration 156, error is 2.256583\n",
      "iteration 157, error is 2.786726\n",
      "iteration 158, error is 2.400963\n",
      "iteration 159, error is 2.371470\n",
      "iteration 160, error is 2.145521\n",
      "iteration 161, error is 2.348835\n",
      "iteration 162, error is 2.718807\n",
      "iteration 163, error is 2.464123\n",
      "iteration 164, error is 2.317041\n",
      "iteration 165, error is 2.639896\n",
      "iteration 166, error is 2.374721\n",
      "iteration 167, error is 3.108068\n",
      "iteration 168, error is 2.657660\n",
      "iteration 169, error is 2.835353\n",
      "iteration 170, error is 2.672085\n",
      "iteration 171, error is 2.556073\n",
      "iteration 172, error is 2.176539\n",
      "iteration 173, error is 2.182763\n",
      "iteration 174, error is 2.352022\n",
      "iteration 175, error is 2.533862\n",
      "iteration 176, error is 1.746950\n",
      "iteration 177, error is 2.084049\n",
      "iteration 178, error is 2.343580\n",
      "iteration 179, error is 1.725313\n",
      "iteration 180, error is 2.495645\n",
      "iteration 181, error is 2.602870\n",
      "iteration 182, error is 2.656255\n",
      "iteration 183, error is 2.320742\n",
      "iteration 184, error is 1.686856\n",
      "iteration 185, error is 2.318999\n",
      "iteration 186, error is 2.293064\n",
      "iteration 187, error is 2.648869\n",
      "iteration 188, error is 2.410765\n",
      "iteration 189, error is 2.899807\n",
      "iteration 190, error is 2.462727\n",
      "iteration 191, error is 1.627719\n",
      "iteration 192, error is 2.041725\n",
      "iteration 193, error is 2.671435\n",
      "iteration 194, error is 2.549968\n",
      "iteration 195, error is 2.667602\n",
      "iteration 196, error is 2.892102\n",
      "iteration 197, error is 2.441776\n",
      "iteration 198, error is 1.794213\n",
      "iteration 199, error is 1.778192\n",
      "iteration 200, error is 2.930611\n",
      "iteration 201, error is 2.807235\n",
      "iteration 202, error is 1.921354\n",
      "iteration 203, error is 2.264469\n",
      "iteration 204, error is 3.094107\n",
      "iteration 205, error is 2.563681\n",
      "iteration 206, error is 2.650772\n",
      "iteration 207, error is 1.713700\n",
      "iteration 208, error is 1.940171\n",
      "iteration 209, error is 2.367711\n",
      "iteration 210, error is 1.922306\n",
      "iteration 211, error is 2.201189\n",
      "iteration 212, error is 3.114203\n",
      "iteration 213, error is 1.417884\n",
      "iteration 214, error is 2.610991\n",
      "iteration 215, error is 2.938104\n",
      "iteration 216, error is 2.560678\n",
      "iteration 217, error is 1.631064\n",
      "iteration 218, error is 1.793955\n",
      "iteration 219, error is 2.425203\n",
      "iteration 220, error is 1.969300\n",
      "iteration 221, error is 2.005266\n",
      "iteration 222, error is 1.920501\n",
      "iteration 223, error is 2.171363\n",
      "iteration 224, error is 2.362624\n",
      "iteration 225, error is 2.486224\n",
      "iteration 226, error is 2.351962\n",
      "iteration 227, error is 2.232833\n",
      "iteration 228, error is 1.482611\n",
      "iteration 229, error is 1.684163\n",
      "iteration 230, error is 2.987004\n",
      "iteration 231, error is 2.296886\n",
      "iteration 232, error is 2.124370\n",
      "iteration 233, error is 1.375240\n",
      "iteration 234, error is 2.069708\n",
      "iteration 235, error is 1.404512\n",
      "iteration 236, error is 1.847338\n",
      "iteration 237, error is 1.590611\n",
      "iteration 238, error is 2.238088\n",
      "iteration 239, error is 2.065562\n",
      "iteration 240, error is 1.899771\n",
      "iteration 241, error is 3.643730\n",
      "iteration 242, error is 1.841914\n",
      "iteration 243, error is 2.027660\n",
      "iteration 244, error is 1.557795\n",
      "iteration 245, error is 2.623978\n",
      "iteration 246, error is 2.271932\n",
      "iteration 247, error is 1.546426\n",
      "iteration 248, error is 1.208742\n",
      "iteration 249, error is 1.295608\n",
      "iteration 250, error is 1.259036\n",
      "iteration 251, error is 1.660710\n",
      "iteration 252, error is 1.606505\n",
      "iteration 253, error is 3.524123\n",
      "iteration 254, error is 0.968660\n",
      "iteration 255, error is 1.751703\n",
      "iteration 256, error is 2.383111\n",
      "iteration 257, error is 0.935570\n",
      "iteration 258, error is 2.442722\n",
      "iteration 259, error is 1.747551\n",
      "iteration 260, error is 2.309074\n",
      "iteration 261, error is 0.893053\n",
      "iteration 262, error is 2.486833\n",
      "iteration 263, error is 1.230132\n",
      "iteration 264, error is 1.708389\n",
      "iteration 265, error is 0.853737\n",
      "iteration 266, error is 2.542523\n",
      "iteration 267, error is 1.703186\n",
      "iteration 268, error is 1.028268\n",
      "iteration 269, error is 2.229310\n",
      "iteration 270, error is 0.804917\n",
      "iteration 271, error is 1.165501\n",
      "iteration 272, error is 2.651104\n",
      "iteration 273, error is 1.271251\n",
      "iteration 274, error is 1.129564\n",
      "iteration 275, error is 1.122149\n",
      "iteration 276, error is 1.247346\n",
      "iteration 277, error is 1.230270\n",
      "iteration 278, error is 1.310567\n",
      "iteration 279, error is 0.725640\n",
      "iteration 280, error is 2.328657\n",
      "iteration 281, error is 1.660506\n",
      "iteration 282, error is 1.439762\n",
      "iteration 283, error is 1.391076\n",
      "iteration 284, error is 1.259527\n",
      "iteration 285, error is 2.025331\n",
      "iteration 286, error is 2.037906\n",
      "iteration 287, error is 1.264065\n",
      "iteration 288, error is 2.025543\n",
      "iteration 289, error is 1.547222\n",
      "iteration 290, error is 1.451933\n",
      "iteration 291, error is 0.822335\n",
      "iteration 292, error is 1.920740\n",
      "iteration 293, error is 1.240586\n",
      "iteration 294, error is 0.770467\n",
      "iteration 295, error is 0.599020\n",
      "iteration 296, error is 1.459750\n",
      "iteration 297, error is 0.770199\n",
      "iteration 298, error is 0.922244\n",
      "iteration 299, error is 0.868175\n",
      "iteration 300, error is 1.539562\n",
      "iteration 301, error is 0.556335\n",
      "iteration 302, error is 0.550503\n",
      "iteration 303, error is 1.479879\n",
      "iteration 304, error is 1.503605\n",
      "iteration 305, error is 1.743670\n",
      "iteration 306, error is 1.003985\n",
      "iteration 307, error is 0.522209\n",
      "iteration 308, error is 0.676383\n",
      "iteration 309, error is 1.153866\n",
      "iteration 310, error is 0.663996\n",
      "iteration 311, error is 1.201175\n",
      "iteration 312, error is 0.667490\n",
      "iteration 313, error is 1.437158\n",
      "iteration 314, error is 1.337003\n",
      "iteration 315, error is 1.105732\n",
      "iteration 316, error is 0.477800\n",
      "iteration 317, error is 1.028116\n",
      "iteration 318, error is 0.637477\n",
      "iteration 319, error is 1.081926\n",
      "iteration 320, error is 1.334075\n",
      "iteration 321, error is 1.724519\n",
      "iteration 322, error is 0.766304\n",
      "iteration 323, error is 1.719014\n",
      "iteration 324, error is 1.278311\n",
      "iteration 325, error is 1.490744\n",
      "iteration 326, error is 0.433393\n",
      "iteration 327, error is 1.044271\n",
      "iteration 328, error is 0.570851\n",
      "iteration 329, error is 0.577374\n",
      "iteration 330, error is 2.019632\n",
      "iteration 331, error is 0.564812\n",
      "iteration 332, error is 0.858940\n",
      "iteration 333, error is 0.552678\n",
      "iteration 334, error is 1.552258\n",
      "iteration 335, error is 1.480015\n",
      "iteration 336, error is 0.390688\n",
      "iteration 337, error is 0.906537\n",
      "iteration 338, error is 0.530519\n",
      "iteration 339, error is 1.649424\n",
      "iteration 340, error is 2.412401\n",
      "iteration 341, error is 0.375068\n",
      "iteration 342, error is 0.513868\n",
      "iteration 343, error is 0.368634\n",
      "iteration 344, error is 1.488201\n",
      "iteration 345, error is 0.504497\n",
      "iteration 346, error is 0.496192\n",
      "iteration 347, error is 0.634905\n",
      "iteration 348, error is 0.487966\n",
      "iteration 349, error is 0.626033\n",
      "iteration 350, error is 0.658490\n",
      "iteration 351, error is 0.478868\n",
      "iteration 352, error is 0.783339\n",
      "iteration 353, error is 1.004297\n",
      "iteration 354, error is 0.463695\n",
      "iteration 355, error is 1.685709\n",
      "iteration 356, error is 1.116138\n",
      "iteration 357, error is 0.809988\n",
      "iteration 358, error is 0.452382\n",
      "iteration 359, error is 1.261089\n",
      "iteration 360, error is 0.625090\n",
      "iteration 361, error is 1.474846\n",
      "iteration 362, error is 0.751458\n",
      "iteration 363, error is 0.611253\n",
      "iteration 364, error is 0.299991\n",
      "iteration 365, error is 1.048756\n",
      "iteration 366, error is 1.170337\n",
      "iteration 367, error is 1.304129\n",
      "iteration 368, error is 1.168480\n",
      "iteration 369, error is 1.645459\n",
      "iteration 370, error is 1.388357\n",
      "iteration 371, error is 1.355742\n",
      "iteration 372, error is 0.883317\n",
      "iteration 373, error is 0.752339\n",
      "iteration 374, error is 1.344786\n",
      "iteration 375, error is 1.537611\n",
      "iteration 376, error is 1.048871\n",
      "iteration 377, error is 1.131584\n",
      "iteration 378, error is 1.353240\n",
      "iteration 379, error is 0.412781\n",
      "iteration 380, error is 0.831770\n",
      "iteration 381, error is 0.407708\n",
      "iteration 382, error is 0.266856\n",
      "iteration 383, error is 0.595688\n",
      "iteration 384, error is 0.393645\n",
      "iteration 385, error is 1.168038\n",
      "iteration 386, error is 0.259126\n",
      "iteration 387, error is 1.484799\n",
      "iteration 388, error is 0.395051\n",
      "iteration 389, error is 1.028840\n",
      "iteration 390, error is 0.725927\n",
      "iteration 391, error is 0.252168\n",
      "iteration 392, error is 0.250216\n",
      "iteration 393, error is 0.382008\n",
      "iteration 394, error is 0.377212\n",
      "iteration 395, error is 0.360293\n",
      "iteration 396, error is 1.270995\n",
      "iteration 397, error is 1.371707\n",
      "iteration 398, error is 0.567834\n",
      "iteration 399, error is 1.362117\n",
      "iteration 400, error is 1.903325\n",
      "iteration 401, error is 0.236706\n",
      "iteration 402, error is 0.234892\n",
      "iteration 403, error is 0.943309\n",
      "iteration 404, error is 0.232935\n",
      "iteration 405, error is 0.806888\n",
      "iteration 406, error is 0.929480\n",
      "iteration 407, error is 1.169198\n",
      "iteration 408, error is 0.360273\n",
      "iteration 409, error is 0.697572\n",
      "iteration 410, error is 0.348722\n",
      "iteration 411, error is 0.679503\n",
      "iteration 412, error is 0.447446\n",
      "iteration 413, error is 0.216294\n",
      "iteration 414, error is 0.795885\n",
      "iteration 415, error is 0.903481\n",
      "iteration 416, error is 0.333381\n",
      "iteration 417, error is 0.213470\n",
      "iteration 418, error is 0.315872\n",
      "iteration 419, error is 0.325605\n",
      "iteration 420, error is 1.295365\n",
      "iteration 421, error is 0.208717\n",
      "iteration 422, error is 0.320980\n",
      "iteration 423, error is 0.671376\n",
      "iteration 424, error is 0.202411\n",
      "iteration 425, error is 1.115495\n",
      "iteration 426, error is 0.200639\n",
      "iteration 427, error is 0.305463\n",
      "iteration 428, error is 0.549703\n",
      "iteration 429, error is 1.099532\n",
      "iteration 430, error is 0.644902\n",
      "iteration 431, error is 1.085383\n",
      "iteration 432, error is 1.187389\n",
      "iteration 433, error is 0.315645\n",
      "iteration 434, error is 1.185550\n",
      "iteration 435, error is 0.316745\n",
      "iteration 436, error is 0.312569\n",
      "iteration 437, error is 2.467102\n",
      "iteration 438, error is 0.309266\n",
      "iteration 439, error is 0.189472\n",
      "iteration 440, error is 1.995726\n",
      "iteration 441, error is 0.390063\n",
      "iteration 442, error is 0.624351\n",
      "iteration 443, error is 1.108278\n",
      "iteration 444, error is 0.284769\n",
      "iteration 445, error is 0.863132\n",
      "iteration 446, error is 0.178882\n",
      "iteration 447, error is 0.749248\n",
      "iteration 448, error is 1.569409\n",
      "iteration 449, error is 1.857354\n",
      "iteration 450, error is 0.512305\n",
      "iteration 451, error is 0.275540\n",
      "iteration 452, error is 0.629494\n",
      "iteration 453, error is 1.212301\n",
      "iteration 454, error is 0.274867\n",
      "iteration 455, error is 0.727187\n",
      "iteration 456, error is 0.267672\n",
      "iteration 457, error is 0.835115\n",
      "iteration 458, error is 1.215184\n",
      "iteration 459, error is 1.107772\n",
      "iteration 460, error is 0.170889\n",
      "iteration 461, error is 0.274925\n",
      "iteration 462, error is 0.271447\n",
      "iteration 463, error is 1.197258\n",
      "iteration 464, error is 0.166171\n",
      "iteration 465, error is 0.264330\n",
      "iteration 466, error is 0.816192\n",
      "iteration 467, error is 1.900689\n",
      "iteration 468, error is 0.164798\n",
      "iteration 469, error is 0.264095\n",
      "iteration 470, error is 0.162173\n",
      "iteration 471, error is 0.258771\n",
      "iteration 472, error is 0.807754\n",
      "iteration 473, error is 0.160422\n",
      "iteration 474, error is 0.159336\n",
      "iteration 475, error is 0.158264\n",
      "iteration 476, error is 0.157208\n",
      "iteration 477, error is 1.428446\n",
      "iteration 478, error is 0.241381\n",
      "iteration 479, error is 0.239830\n",
      "iteration 480, error is 0.254985\n",
      "iteration 481, error is 0.155421\n",
      "iteration 482, error is 0.249808\n",
      "iteration 483, error is 0.153008\n",
      "iteration 484, error is 0.244844\n",
      "iteration 485, error is 0.784674\n",
      "iteration 486, error is 0.151516\n",
      "iteration 487, error is 0.231003\n",
      "iteration 488, error is 0.149907\n",
      "iteration 489, error is 0.148923\n",
      "iteration 490, error is 0.147953\n",
      "iteration 491, error is 1.237560\n",
      "iteration 492, error is 0.240420\n",
      "iteration 493, error is 0.316651\n",
      "iteration 494, error is 0.314416\n",
      "iteration 495, error is 0.552246\n",
      "iteration 496, error is 0.143280\n",
      "iteration 497, error is 0.678364\n",
      "iteration 498, error is 0.641616\n",
      "iteration 499, error is 0.141174\n",
      "weight:\n",
      "[[-0.47246911  0.17424846  0.30980188]\n",
      " [ 0.31398416 -1.34875864  3.65952717]\n",
      " [-1.78384204 -3.23517555  1.84527187]\n",
      " [ 1.41726079  3.37519512 -1.54080033]\n",
      " [-0.73049696  1.58889251 -3.23771276]]\n",
      "hidden_data:\n",
      "[[ 0.  1.]]\n",
      "visible_data:\n",
      "[[ 1.  0.  1.  0.]]\n",
      "推荐得分:\n",
      "0 0.975200256273\n",
      "1 0.828813040554\n",
      "2 0.211158397982\n",
      "3 0.0355508596295\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "__author__ = \"yanbin\"\n",
    " \n",
    "import numpy as np\n",
    " \n",
    "class RBM(object):\n",
    " \n",
    "    def __init__(self, num_visible, num_hidden, learn_rate=0.1, learn_batch=1000):\n",
    "        self.num_visible = num_visible  # 可视层神经元个数\n",
    "        self.num_hidden = num_hidden    # 隐藏层神经元个数\n",
    "        self.learn_rate = learn_rate    # 学习率\n",
    "        self.learn_batch = learn_batch  # 每次根据多少样本进行学习\n",
    " \n",
    "        '''初始化连接权重'''\n",
    "        self.weights = 0.1 * \\\n",
    "            np.random.randn(self.num_visible,\n",
    "                            self.num_hidden)  # 依据0.1倍的标准正太分布随机生成权重\n",
    "        # 第一行插入全0，即偏置和隐藏层的权重初始化为0\n",
    "        self.weights = np.insert(self.weights, 0, 0, axis=0)\n",
    "        # 第一列插入全0，即偏置和可视层的权重初始化为0\n",
    "        self.weights = np.insert(self.weights, 0, 0, axis=1)\n",
    " \n",
    "    def _logistic(self, x):\n",
    "        '''直接使用1.0 / (1.0 + np.exp(-x))容易发警告“RuntimeWarning: overflowencountered in exp”，\n",
    "           转换成如下等价形式后算法会更稳定\n",
    "        '''\n",
    "        return 0.5 * (1 + np.tanh(0.5 * x))\n",
    " \n",
    "    def train(self, rating_data, max_steps=1000, eps=1.0e-4):\n",
    "        '''迭代训练，得到连接权重\n",
    "        '''\n",
    "        for step in xrange(max_steps):  # 迭代训练多少次\n",
    "            error = 0.0  # 误差平方和\n",
    "            # 每次拿一批样本还调整权重\n",
    "            for i in xrange(0, rating_data.shape[0], self.learn_batch):\n",
    "                num_examples = min(self.learn_batch, rating_data.shape[0] - i)\n",
    "                data = rating_data[i:i + num_examples, :]\n",
    "                data = np.insert(data, 0, 1, axis=1)  # 第一列插入全1，即偏置的值初始化为1\n",
    " \n",
    "                pos_hidden_activations = np.dot(data, self.weights)\n",
    "                pos_hidden_probs = self._logistic(pos_hidden_activations)\n",
    "                pos_hidden_states = pos_hidden_probs > np.random.rand(\n",
    "                    num_examples, self.num_hidden + 1)\n",
    "                # pos_associations=np.dot(data.T,pos_hidden_states)         #对隐藏层作二值化\n",
    "                pos_associations = np.dot(\n",
    "                    data.T, pos_hidden_probs)  # 对隐藏层不作二值化\n",
    " \n",
    "                neg_visible_activations = np.dot(\n",
    "                    pos_hidden_states, self.weights.T)\n",
    "                neg_visible_probs = self._logistic(neg_visible_activations)\n",
    "                neg_visible_probs[:, 0] = 1  # 强行把偏置的值重置为1\n",
    "                neg_hidden_activations = np.dot(\n",
    "                    neg_visible_probs, self.weights)\n",
    "                neg_hidden_probs = self._logistic(neg_hidden_activations)\n",
    "                # neg_hidden_states=neg_hidden_probs>np.random.rand(num_examples,self.num_hidden+1)\n",
    "                # neg_associations=np.dot(neg_visible_probs.T,neg_hidden_states)      #对隐藏层作二值化\n",
    "                neg_associations = np.dot(\n",
    "                    neg_visible_probs.T, neg_hidden_probs)  # 对隐藏层不作二值化\n",
    " \n",
    "                # 更新权重。另外一种尝试是带冲量的梯度下降，即本次前进的方向是本次梯度与上一次梯度的线性加权和（这样的话需要额外保存上一次的梯度）\n",
    "                self.weights += self.learn_rate * \\\n",
    "                    (pos_associations - neg_associations) / num_examples\n",
    " \n",
    "                # 计算误差平方和\n",
    "                error += np.sum((data - neg_visible_probs)**2)\n",
    "            if error < eps:  # 所有样本的误差平方和低于阈值于终止迭代\n",
    "                break\n",
    "            print 'iteration %d, error is %f' % (step, error)\n",
    " \n",
    "    def getHidden(self, visible_data):\n",
    "        '''根据输入层得到隐藏层\n",
    "           visible_data是一个matrix，每行代表一个样本\n",
    "        '''\n",
    "        num_examples = visible_data.shape[0]\n",
    "        hidden_states = np.ones((num_examples, self.num_hidden + 1))\n",
    "        visible_data = np.insert(visible_data, 0, 1, axis=1)  # 第一列插入偏置\n",
    "        hidden_activations = np.dot(visible_data, self.weights)\n",
    "        hidden_probs = self._logistic(hidden_activations)\n",
    "        hidden_states[:, :] = hidden_probs > np.random.rand(\n",
    "            num_examples, self.num_hidden + 1)\n",
    "        hidden_states = hidden_states[:, 1:]            # 即首列删掉，即把偏置去掉\n",
    "        return hidden_states\n",
    " \n",
    "    def getVisible(self, hidden_data):\n",
    "        '''根据隐藏层得到输入层\n",
    "           hidden_data是一个matrix，每行代表一个样本\n",
    "        '''\n",
    "        num_examples = hidden_data.shape[0]\n",
    "        visible_states = np.ones((num_examples, self.num_visible + 1))\n",
    "        hidden_data = np.insert(hidden_data, 0, 1, axis=1)\n",
    "        visible_activations = np.dot(hidden_data, self.weights.T)\n",
    "        visible_probs = self._logistic(visible_activations)\n",
    "        visible_states[:, :] = visible_probs > np.random.rand(\n",
    "            num_examples, self.num_visible + 1)\n",
    "        visible_states = visible_states[:, 1:]\n",
    "        return visible_states\n",
    " \n",
    "    def predict(self, visible_data):\n",
    "        num_examples = visible_data.shape[0]\n",
    "        hidden_states = np.ones((num_examples, self.num_hidden + 1))\n",
    "        visible_data = np.insert(visible_data, 0, 1, axis=1)  # 第一列插入偏置\n",
    "        '''forward'''\n",
    "        hidden_activations = np.dot(visible_data, self.weights)\n",
    "        hidden_probs = self._logistic(hidden_activations)\n",
    "        # hidden_states[:, :] = hidden_probs > np.random.rand(\n",
    "        #     num_examples, self.num_hidden + 1)\n",
    "        '''backward'''\n",
    "        visible_states = np.ones((num_examples, self.num_visible + 1))\n",
    "        # visible_activations = np.dot(hidden_states, self.weights.T)  #对隐藏层作二值化\n",
    "        visible_activations = np.dot(hidden_probs, self.weights.T)  # 对隐藏层不作二值化\n",
    "        visible_probs = self._logistic(visible_activations)  # 直接返回可视层的概率值\n",
    " \n",
    "        return visible_probs[:, 1:]  # 把第0列(偏置)去掉\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    rbm = RBM(num_visible=4, num_hidden=2, learn_rate=0.1, learn_batch=1000)\n",
    "    rating_data = np.array([[1, 1, 1, 0, 0, 0], [1, 0, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [\n",
    "                           0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 0, 0], [0, 0, 1, 1, 1, 0]])\n",
    "    rating_data = np.array([[1,1,0,0],[1,0,1,0],[0,0,1,1]])\n",
    "    rbm.train(rating_data,max_steps=500, eps=1.0e-4)\n",
    "    print 'weight:\\n', rbm.weights\n",
    "    rating = np.array([[0, 0, 0, 0.9, 0.7, 0]])  # 评分需要做归一化。该用户喜欢第四、五项\n",
    "    rating = np.array([[1,1,0,0]])\n",
    "    hidden_data = rbm.getHidden(rating)\n",
    "    print 'hidden_data:\\n', hidden_data\n",
    "    visible_data = rbm.getVisible(hidden_data)\n",
    "    print 'visible_data:\\n', visible_data\n",
    "    predict_data = rbm.predict(rating)\n",
    "    print '推荐得分:'\n",
    "    for i, score in enumerate(predict_data[0, :]):\n",
    "        print i, score  # 第三、四、五项的推荐得分很高，同时用户已明确表示过喜欢四、五，所以我们把第三项推荐给用户\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
